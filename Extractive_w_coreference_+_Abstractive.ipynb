{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extractive w/ coreference + Abstractive.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPYWhARdVgafNilbnUjcjMW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulowoicho/msc_project/blob/master/Extractive_w_coreference_%2B_Abstractive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMbS7sPh_ErP",
        "colab_type": "text"
      },
      "source": [
        "T5 and Bart did well. So did Bert + Kmeans with coreference. What happens when we use both."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dur-bDxr_hlM",
        "colab_type": "text"
      },
      "source": [
        "#Standard installation and imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHNKija7CTAo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d0c77242-30d4-4585-f367-fe5abea66be2"
      },
      "source": [
        "#set up again because restarted runtime\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "project_id = 'test-281700'\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "#download model..restarted runtime\n",
        "bucket_name = 'spotify_asr_dataset'\n",
        "!gsutil -m cp -r gs://{bucket_name}/t5-model-3000.zip /content/\n",
        "!unzip t5-model-3000.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "Copying gs://spotify_asr_dataset/t5-model-3000.zip...\n",
            "| [1/1 files][787.9 MiB/787.9 MiB] 100% Done  39.6 MiB/s ETA 00:00:00           \n",
            "Operation completed over 1 objects/787.9 MiB.                                    \n",
            "Archive:  t5-model-3000.zip\n",
            "   creating: t5-model-3000/\n",
            "  inflating: t5-model-3000/pytorch_model.bin  \n",
            " extracting: t5-model-3000/tokenizer_config.json  \n",
            "  inflating: t5-model-3000/config.json  \n",
            "  inflating: t5-model-3000/spiece.model  \n",
            "  inflating: t5-model-3000/special_tokens_map.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG5uCvQouwOQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "16868313-fa5b-4ce6-c0c2-edb9d9198fa8"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install rouge\n",
        "!pip install neuralcoref==4.0\n",
        "!pip install spacy==2.1.0 #notebook crashes on other versions\n",
        "!python -m spacy download en\n",
        "\n",
        "!git clone https://github.com/paulowoicho/bert-extractive-summarizer.git\n",
        "!mv bert-extractive-summarizer summarizer"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 15.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 17.9MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 41.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=6a180310e497074d540cd3e215cccf1dd0b768906ad81e22522910dccca0d22e\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n",
            "Collecting neuralcoref==4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/24/0ec7845a5b73b637aa691ff4d1b9b48f3a0f3369f4002a59ffd7a7462fdb/neuralcoref-4.0-cp36-cp36m-manylinux1_x86_64.whl (287kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref==4.0) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from neuralcoref==4.0) (1.14.30)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref==4.0) (1.18.5)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref==4.0) (2.2.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2020.6.20)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->neuralcoref==4.0) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.30 in /usr/local/lib/python3.6/dist-packages (from boto3->neuralcoref==4.0) (1.17.30)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->neuralcoref==4.0) (0.10.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (0.7.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (49.2.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (3.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (1.1.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.30->boto3->neuralcoref==4.0) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.30->boto3->neuralcoref==4.0) (2.8.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.1.0->neuralcoref==4.0) (1.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.30->boto3->neuralcoref==4.0) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.1.0->neuralcoref==4.0) (3.1.0)\n",
            "Installing collected packages: neuralcoref\n",
            "Successfully installed neuralcoref-4.0\n",
            "Collecting spacy==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/39/4bde5da5f18ab0bdd525760c4fe38808b4bb03907a2aea094000d831afe1/spacy-2.1.0-cp36-cp36m-manylinux1_x86_64.whl (27.7MB)\n",
            "\u001b[K     |████████████████████████████████| 27.7MB 111kB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (0.7.1)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 38.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (2.6.0)\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.1MB/s \n",
            "\u001b[?25hCollecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (2.23.0)\n",
            "Collecting thinc<7.1.0,>=7.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/a5/9ace20422e7bb1bdcad31832ea85c52a09900cd4a7ce711246bfb92206ba/thinc-7.0.8-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 34.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (1.18.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (2.0.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.0) (4.41.1)\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: blis, preshed, plac, thinc, spacy\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: preshed 3.0.2\n",
            "    Uninstalling preshed-3.0.2:\n",
            "      Successfully uninstalled preshed-3.0.2\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed blis-0.2.4 plac-0.9.6 preshed-2.0.1 spacy-2.1.0 thinc-7.0.8\n",
            "Collecting en_core_web_sm==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1MB 1.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.1.0-cp36-none-any.whl size=11074435 sha256=d60d53e9586d342cb4206dd5a3c4a14da36aee462615b95c6cc5b86e676b1888\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_798v7mu/wheels/39/ea/3b/507f7df78be8631a7a3d7090962194cf55bc1158572c0be77f\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Cloning into 'bert-extractive-summarizer'...\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 378 (delta 0), reused 3 (delta 0), pack-reused 374\u001b[K\n",
            "Receiving objects: 100% (378/378), 85.48 KiB | 2.95 MiB/s, done.\n",
            "Resolving deltas: 100% (215/215), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6htGWio_4JW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e19c275e-a1bf-4255-b62f-a423939fccf2"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from summarizer import Summarizer\n",
        "from summarizer.coreference_handler import CoreferenceHandler\n",
        "from transformers import *\n",
        "from rouge import Rouge\n",
        "import spacy\n",
        "import neuralcoref"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 40155833/40155833 [00:00<00:00, 43455822.07B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1ixRR7FA-75",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fc300ede-fcb1-4578-b78e-4403f87a29fd"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvgcWuUNAanf",
        "colab_type": "text"
      },
      "source": [
        "#Spanbert + t5 pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV3zThUjBRRc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "0573c511-17dc-4c50-9c02-191d84ec3b3c"
      },
      "source": [
        "#Spanbert\n",
        "handler = CoreferenceHandler(greedyness=.4)\n",
        "custom_config = AutoConfig.from_pretrained(\"SpanBERT/spanbert-base-cased\")\n",
        "custom_config.output_hidden_states=True\n",
        "custom_tokenizer = AutoTokenizer.from_pretrained(\"SpanBERT/spanbert-base-cased\")\n",
        "custom_model = AutoModel.from_pretrained(\"SpanBERT/spanbert-base-cased\", config=custom_config)\n",
        "\n",
        "spanbert_model = Summarizer(custom_model = custom_model, sentence_handler=handler, custom_tokenizer=custom_tokenizer)\n",
        "\n",
        "def span_bert(transcript, ratio):\n",
        "  result = spanbert_model(transcript, min_length=60, ratio=ratio)\n",
        "  return ''.join(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:transformers.modeling_utils:Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-base-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9UN_WBgAqPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#t5\n",
        "tokenizer = T5Tokenizer.from_pretrained('/content/t5-model-3000')\n",
        "model = T5ForConditionalGeneration.from_pretrained('/content/t5-model-3000')\n",
        "\n",
        "\n",
        "def t5_inference(transcript, threshold):\n",
        "  t5_form = 'summarize: ' + transcript\n",
        "  tokenized_text = tokenizer.encode(t5_form, return_tensors=\"pt\")\n",
        "  if len(tokenized_text[0]) > threshold:\n",
        "    #run out of RAM/crashes on large number of tokens\n",
        "    revised_text = sent_tokenize(t5_form)\n",
        "    length = len(revised_text)\n",
        "    final_text = revised_text[:int(length/2)] #maybe they talk about content in the first half? find proof\n",
        "    text = ' '.join(final_text)\n",
        "    return t5_inference(text)\n",
        "  summary_ids = model.generate(tokenized_text, max_length=150, num_beams=2, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\n",
        "  output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "  return output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW6CwOYTBu2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def spanbert_t5(transcript, ratio, threshold):\n",
        "  important_sentences = span_bert(transcript, ratio)\n",
        "  summary = t5_inference(important_sentences, threshold)\n",
        "  return summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNLC73QSCu3o",
        "colab_type": "text"
      },
      "source": [
        "#Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZISZgaXCK82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "reference_results = pd.read_csv('150_gold.csv')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qwayo6DcC8oN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "2080fa01-1772-40f8-8a55-3ae61fe38a16"
      },
      "source": [
        "ext_abs = reference_results[['episode_id', 'transcript', 'episode_description']]\n",
        "ext_abs.head(2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>episode_id</th>\n",
              "      <th>transcript</th>\n",
              "      <th>episode_description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>spotify:episode:08hXUWN6aOnHULXrqMiwTi</td>\n",
              "      <td>Hello everybody. What's going on in this Jess...</td>\n",
              "      <td>If you want to start mastering recruiting whic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>spotify:episode:0CExTNH4LFqp1ec1mhTd4I</td>\n",
              "      <td>Ladies, have you been molested don't be a vic...</td>\n",
              "      <td>Don't be a silent victim of crime. a parody fr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               episode_id  ...                                episode_description\n",
              "0  spotify:episode:08hXUWN6aOnHULXrqMiwTi  ...  If you want to start mastering recruiting whic...\n",
              "1  spotify:episode:0CExTNH4LFqp1ec1mhTd4I  ...  Don't be a silent victim of crime. a parody fr...\n",
              "\n",
              "[2 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0FL5258DSWj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "779f4418-dc9f-431b-d84b-3c63b190de77"
      },
      "source": [
        "ext_abs['auto_summary'] = ext_abs.apply(lambda row: spanbert_t5(row['transcript'], 0.1, 7000), axis = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (853 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (697 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (800 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (781 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (881 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (1144 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (774 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (758 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (744 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (578 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (692 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (650 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (735 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (766 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (939 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (971 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (670 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (783 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (649 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (697 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (694 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (672 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (1421 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (897 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (920 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (927 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (723 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (603 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (1035 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (1117 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (894 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (1143 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (696 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (702 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (1022 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (763 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (1196 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (724 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (797 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (770 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (1093 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (779 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (876 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (1004 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (695 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (839 > 512). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcWAQpenwXiF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "7ed6ef64-b69b-4290-a799-89490526e9c8"
      },
      "source": [
        "ext_abs.iloc[1]['auto_summary']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ladies, have you been molested don’t be a victim of crime here are some things you can do when it’s late. Try your best not to be molested by a culprit from a good University who has good academic results and the potential to excel in life. When it’s late try your best not to be molested by a culprit from a good University who has good academic results and the potential to excel in life. If you’ve been molested, try your best not to be'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT0-V5IWw-BL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c53865a4-20db-4097-9e0f-c0eca495bac9"
      },
      "source": [
        "ext_abs.iloc[1]['episode_description']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Don't be a silent victim of crime. a parody from mrbrown.com \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFu8vgJrEAb5",
        "colab_type": "text"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkjkwEyOv28D",
        "colab_type": "text"
      },
      "source": [
        "##spanbert + t5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpgNDCV5GSwl",
        "colab_type": "text"
      },
      "source": [
        "Just realized I initially did not use Rouge API properly. So I may need to recompute the other results I had gathered, depending on these following results :(. On the other hand, it just means recall and precision scores are switched, so f measure should be the same"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_QUApEnENTh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1dede8b4-c67c-4e95-b97a-9757ce835057"
      },
      "source": [
        "#what i have been doing\n",
        "rouge = Rouge()\n",
        "rouge_scores_w = rouge.get_scores(ext_abs['episode_description'], ext_abs['auto_summary'], avg=True)\n",
        "rouge_scores_w['rouge-1']['f'], rouge_scores_w['rouge-2']['f'], rouge_scores_w['rouge-l']['f']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.16932254852889728, 0.0723673729650572, 0.1957123945829847)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2820LJsE7OM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a83cbaa4-f355-4215-81d0-16e01e1324da"
      },
      "source": [
        "#correct version\n",
        "rouge_scores_r = rouge.get_scores(ext_abs['auto_summary'], ext_abs['episode_description'], avg=True)\n",
        "rouge_scores_r['rouge-1']['f'], rouge_scores_r['rouge-2']['f'], rouge_scores_r['rouge-l']['f']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.16932254852889728, 0.0723673729650572, 0.19618320623105837)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "933W-DMkRb0E",
        "colab_type": "text"
      },
      "source": [
        "All is well :). This is better than all extractive techniques but worse than supervised BART and T5. It is better than semi-supervised bart though. However, this depends on the hyperparameter I used (0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yKFa9wtvUjC",
        "colab_type": "text"
      },
      "source": [
        "#First N and t5 no coreference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXG6XltOwM-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Forgot first-N again. Rather than rewriting functions, I could make it so that is is a super function that takes\n",
        "#other functions as parameters\n",
        "\n",
        "def first_n(transcript, threshold):\n",
        "  sentences = sent_tokenize(transcript)\n",
        "  return ' '.join(sentences[:threshold])\n",
        "\n",
        "def firstn_t5(transcript, limit, threshold):\n",
        "  sentences = first_n(transcript, limit)\n",
        "  summary = t5_inference(sentences, threshold)\n",
        "  return summary"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QELxYbO03VE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "f7e6bc9e-dfe2-4d94-e98d-1d57d6e95a80"
      },
      "source": [
        "ext_abs['firstn_summary'] = ext_abs.apply(lambda row: firstn_t5(row['transcript'], 15, 7000), axis = 1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAdbLjnW1Unm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db7484dc-c841-4346-e0d6-c30faa335c66"
      },
      "source": [
        "rouge = Rouge()\n",
        "rouge_scores = rouge.get_scores(ext_abs['firstn_summary'], ext_abs['episode_description'], avg=True)\n",
        "rouge_scores['rouge-1']['f'], rouge_scores['rouge-2']['f'], rouge_scores['rouge-l']['f']"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.1909758054021582, 0.0962574068499044, 0.2229754751606156)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6n3EecIt10T",
        "colab_type": "text"
      },
      "source": [
        "Very surprising results! As good as t5 alone! Although no coreference was used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a3pyCtCzXz_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "2f33528a-5bee-4ec3-d3c6-722e5d404031"
      },
      "source": [
        "ext_abs.iloc[10]['firstn_summary']"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Nick and Jeff discuss college football news on the college football news podcast. --- This episode is sponsored by  Anchor: The easiest way to make a podcast. https://anchor.fm/app Support this podcast: https://anchor.fm/collegefootballnews/support this podcast: https://anchor.fm/collegefootballnews/support this podcast: https://anchor.fm/collegefootballnews/support this podcast: https://anchor.fm/collegefootballnews/support this podcast: https://anchor.fm/collegefootballnews/support this podcast: https://anchor.fm/collegefoot'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48ArnuUszzsF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "b63a3f21-96f4-42de-ab6a-efe279033ba8"
      },
      "source": [
        "ext_abs.iloc[10]['episode_description']"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\xa0 Nick Shepkowski sits down with Jeff Feyerer of College Football News, Inside the Pylon and various other football outlets to preview the 2019 season. \\xa0 Clemson/Alabama round 5 - who has the upper hand entering 2019? How does the Big Ten stack up to the SEC? \\xa0Who are a few underdog picks that could surprise and win their conferences? Predictions, opinions and in depth preview of what the 2019 college football season may bring. \\xa0Enjoy!  ---   This episode is sponsored by  · Anchor: The easiest way to make a podcast.  https://anchor.fm/app  Support this podcast: https://anchor.fm/cfnpodcast/support'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b13x0JS60g4H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "663833a6-1dc2-4410-bc58-91472af0af22"
      },
      "source": [
        "ext_abs.iloc[10]['transcript']"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Hey everybody, it's Bevin. Welcome to Bevin FM over 40 and her friends podcast. This is me, Bevin. I've said my name three times. It's time to start the show. I'm so excited that you've tuned in welcome today. I'm going to talk about some big stuff going on. So first of all, I want to talk about some big changes happening in my life. And I'm going to talk about some of the key learnings that I got when I took a social media Hiatus for four months. So first and foremost I am  Announce this at the top of the last episode that I am moving next month this month. I'm moving in three weeks. Basically I am getting rid of 85% of what I own. This is a made-up estimate of this is just based on how much space I have available to me to store my possessions for when I get another place to live that as my own. But basically I am subleasing a tiny house down the street from my mom and because it's like TurnKey, right?  Please I will show up with me my cat which is the most important thing is that I get to keep my cat all of his needs and he's a special needs cats. He's got a lot of meds and things he needs all of my toiletries, which is not a small than ever. It's not small situation. My mom gave me this toiletries carrying case once it's like truly for like full-size bottles of things and it has proved I was it was in the giveaway pile until I realize I was moving and then I pulled it out of the giveaway pile because I  like actually this is gonna be my bathroom for the next six months because it's going to store all of my full-size products because you know, she this is a I believe in natural beauty, but I also believe in doing what you need to do to take great care of yourself and I take great care of myself. And so all my bathroom stuff is just when then closed just like season will close because I'm moving someplace that has a proper fall proper winter proper Cold Spring and I'm moving very close to the border of Canada. Actually I can see Canada from  Tiny house. I've never actually looked at the view intentionally from this house that I'm renting but I have looked extensively At This Woman's Garden. So I'm actually really excited to go and get to just like really get intimate with that that space I yeah, so like and then I'll probably bring some jigsaw puzzles and some art to work on in the winter, but my plan is to move into this tiny house have my living expenses basically reduced.  Bye.  It's like whatever 5/6 is in percent. I'm going down five six and how much my living expenses are maybe even more than that because you know the power bills and the water bills at a house that has a front yard and backyard or not a joke. So anyway, I'm just like basically doing a fire sale of all of my belongings. I have found just you know, so far Craigslist is over. It's all about Facebook Marketplace. And since I've been selling on Facebook Marketplace things have been  Really quickly. I was trying to sell a bunch of stuff on Craigslist before because I didn't really want all my friends in my business. And now I realize that was foolhardy. I should have been selling on Facebook all along but you know, I kind of like how I got to the decision is really like, I think I know I think I just needed to really give it a shot to stay in La I prayed for guidance and in May I was just like praying for guidance. Like what's my next step? And what do I do?  And do I need to stay in LA or do I leave LA and I got the guidance that I needed to stay in La for at least six months and November 1st will be six months from right about when that decision came through. So where that call came through and so that seems about right. I have no reason to stay in La like nothing is keeping me here. I tried so hard to get a job. I tried so hard to get a roommate. I tried so hard at short-term rentals of my extra room my house and just like nothing gel.  Thing came to fruition. I had a lot of last-minute Miracles that made me able to pay my rent for the last six months and just like barely ich by I've been eating on less than five dollars a day and when you're mostly trading all of your time for money eating I'm less than five dollars a day is not decadent and it's hard like because I can eat pretty well on a budget if I have time to cook, but if I don't have time to cook then it's a lot of like rice-based meals and which is fine like and five dollars a day.  Is still a lot actually for a lot of people's food budgets and but like, you know, when you can't have convenience food or convenience food isn't really available to you and your in your your time schedule it like it's just I feel like I've been incomplete survival mode for the last six seven months and I'm tired and I can't do it anymore like and there is nothing in La that's keeping me here in order to pay these exorbitant rents. And like I just can't afford to keep this house in this house was like why I want to too  Stay in La I was like, I'll stay here if I can stay in this really awesome house that I love so much. So it's like it feels in many ways like my therapist. Who by the way, I'm so sad. I don't get to keep my therapist because it's like we do remote sessions over the phone, but he's licensed in California and he won't get a Washington license just to counsel me. So I'm hopeful that we can have a little Bridge time. So I don't just like move and have no therapy support because therapy is actually been really helpful for me.  But he did remind me before before I made this decision like in that last week where I was like, what's the miracle going to be? How am I going to pay this rent? And he was like you were in this situation because of your breakup. This is not a personal failure of yours. This is just a circumstance of where your life has been and you can't blame yourself for it which like I'm a Capricorn. It's my it's my nature to blame myself for everything and every shortcoming and so I've recently learned that one of the best things that Capricorn  Do is adopt some of those Leo characteristics where Leo's are really about like being proud of their accomplishments and like knowing how far they've come and I think as a Capricorn, you know, it's the symbol of that goat climbing up the mountain and sometimes I get really caught up with where the next point and the mountain is instead of turning around and looking at how far I've come and I'm really proud of myself for making it work as long as I did and I'm really proud of myself for like continuing to connect with other human beings.  Beings create Circles of people around me who are supportive of the mindsets. I want to have and like who I'm becoming in my life and seeking out spiritual Community here in La when I knew that was lacking after my breakup and you know, really giving it a shot and I've Loved this town and like I was driving Rideshare. I was doing lift as a driver for a long time like 50 60 hour weeks doing lift and I  Even make my ends meet working that hard and that is really sad and I think it just kind of goes to show how underpaid Rideshare drivers are by the way always tip your driver like always it's a thing. It's like it's like you tip at a restaurant tip your driver the take-home pay from lift. By the way, if you're thinking of doing it is about 10 bucks an hour before taxes after expenses and before tips. So just just so you know, that's what the person's making who's driving around. They are not making I made about thirty percent of  every Fair which like lift. I don't know. There's something someone Googled in the back of my left base of well on Google. It says blah blah blah, but I want you to remind I want to remind you that I mean and maybe it's just true in La that they're taking about 75% off the top but about 7075 percent. It's so bananas. Sometimes I'll drive someone and I'll make two dollars and sixty two cents and they spent $10 on that red. It is cuckoo anyway, so just to say tip your Rideshare drivers and it's not  Is sustainable income and less you can work 50 to 60 hours a week and even then kind of depending on how heavy expenses are I still have like basically I'm listening to what my body says and my body told me I couldn't drive Rideshare anymore. Like I have this ongoing muscle ache that's in the full back of my leg and my foot is still pretty injured the my driving foot. So it's just from like that. Stop start. Stop start mom traffic e days, so I gave it a shot.  I can safely I really tried to be an ally and it wasn't even like I was trying to be an actress or something like that something. That was La specific. I was just trying to like keep the great house that I loved and try to like thrive in this life that was set up for me when I had a partner whose business made over a hundred fifty thousand dollars a year and I helped her build that business and you know, it is what it is. Like wish we wish we had gotten married and I got alimony, but kind of glad we didn't get married so that this can just be over.  And so now I depart la for the border of Canada. I'm going to live down the street from my mother which I'm actually really excited about I love my mom. She and I have the best relationship we've had in our whole lives. Like I think that's due in large part to both of us having a lot of personal growth. I also she also recently sent me her astrological chart and you really know you're cool with me if I'm like. Hey, what's your birth data so I can look up your astrological chart and so like learning more about  my mom's stuff and how like that it just it all made sense. Once I saw it. I was like, oh my God a Sagittarius moon. Oh my God, right like my mom is amazing. And I'm glad to just get the opportunity to see what it's like to live down the street from her. I've never had that experience. I always kind of thought it was weird when people stayed in the same town as their parents because like I moved away when I was 17 years old. I have not ever lived at home again, and I'm technically not living at home because I didn't grow up where my mom lives now.  She retired up to the Olympic National Forest. She literally lives in the forest and a little tiny village. And so and most of the people leave during the winter. So it's going to be really isolated. I'm hoping to get a great job that like pays my bills and that I can leave at work and I'm going to keep building. I have a couple businesses obviously, you know about faka dance party aerobics, which is my aerobics class that I created for everybody who's left behind by mainstream Fitness and  it hasn't been something I can devote a ton of time on lately. So I've been needing to trade my time for money and the business is not at a point where it can support me financially full-time. So I've been focusing mostly just on my online Pursuits with it. So I've been online weekly aerobics class that you can get through. My patreon patreon is a membership site. It's like crowdfunding meets a membership subscription and for 25 bucks a month. You can take my weekly online aerobics best. And so that has been what I've been  Focusing on because I want it to be able to reach as many people as possible. And this was before I even was thinking that I was going to move away. But now I'm glad like I've been investing in that so that is still ongoing. Its I've never stopped. I've been doing this class every week for the last like since I launched in January, so keep on teaching was online or hybrid class join me anytime you get four classes available to you every month of different lengths. So there's a 10-minute a 20-minute a 45  Minute and a 60 Minute class and I love doing it. I love teaching aerobics the other day. I taught my last La class for a while to a couple of my regulars from LA and it was just like I was like, I love teaching this class. I love doing this. Yeah, and I also have my online workout videos which so funny because my I think the episode 4 is about spirituality and I talked at length about Kyle cease and how his stuff has really he's like a teacher Guru.  And he's really taught me a lot just by being a member of his absolutely everything community and I was I got picked to be on his weekly call which is kind of a big deal and I got picked from my question which it was like basically, how do I know when it's time to quit like, I'm really working hard to stay in this house, but everything seems like it's not working and he didn't actually answer my question, but he like loved the fact that dance party stuff and he loved Frankie dance party and he was like you have to do this.  Have to share this like blah blah blah. So I'm also really not interested in having investors. So it's like something that's got to be built on the side. So I've got that I am intending to continue sharing back a dance party and figuring out what's next but trying to work smarter not harder because I spent a lot of time the last couple of years like when I had the privilege of having someone paying my rent then I'm grateful that I was able to build it, but I was able to kind of try a lot of stuff and  screw up a lot of stuff and like make mistakes and I didn't have a mentor helping me like build it because I was just kind of making it up and like figuring stuff out and like that's I really just saw lack of mentorship in my life like affecting like how I was able to move it forward and now yeah, so now like I see because of the people I got connected to and the the learnings I've done in the last six months staying in La I like I see clearly why  To be here for this time. And also I just don't see staying which is sad because like there are some some people that I did get connected to who are like phenomenal and I would love friends if we talk on my offline. I will tell you all about it, but basically like changed the course of my life changed the trajectory really helping me move some specific personal gross stuff and I'm going to aggressively work to pay off debt. I haven't fully totaled all of my debt, but I think my debt is in the neighborhood of $150,000.  Out of which is student debt left over from a law degree. I don't even use anymore because I you know, just. Lawyers have four times the national rate of suicide and I am not interested in being a lawyer. So I that proved out to me every single time. I've had a lot of job. I've been more suicidal then when I don't and the last time I had a nine-to-five la job which you know, a lot of job is like more like I would say it's like 9 to 9 and every time I was communicating to that  I was on the subway. This is when I was in New York and I would just contemplating suicide was contemplating jumping on the track every time and I never equated like oh this bullying environment that's like argumentative environment and never equated that to like my mental health and why I was having those ideations and now I just like have way more of an awareness of myself. And so anyway, I don't know what industry I'm going to go into. I'm just keeping an open mind and looking for that and like working on building a whole nother business.  This that I'm not even public about and that is my retirement plan and my hope is to retire up where my mom lives and my hope is to retire in the next few years. And you know, that doesn't mean I'll ever stop doing that dance party because I think that's my heart work and I think that's where I meant to serve on a global scale and help sort of shift The Narrative around dance aerobics and fat people Fitness and fat people. And also I'd love it. I love doing dance aerobics and I love sharing it with people.  And I want to you know live to be supported but I don't I would love to be supported by faculty and party aerobics and I haven't like fully like given up. Hope on that but I'm certainly not like hinging on it because there's just so many things that go into whether or not a business is profitable. And so I'm going to get a bill paying job keep working on my retirement business and keep pushing forward FAQ a dance party. That is also a specific question that I was asked by a friend when I was catching.  Hang up today is like what are you doing effective dance party? So rest assured I will still be filming my weekly classes probably with some of my mom's retirement buddies. Hopefully being in the background of my videos and them probably I don't know that I can record aerobics in a tiny house, but they have like a lodge that I can probably use and anyway, so I'm excited to share with you from there. And obviously I'm going to continue doing this podcast and so you'll hear it.  About my life. And yeah, so I'm excited about just the possibilities obviously like opening up for me because I'm willing to make so much change. This is a thing I've learned from Kyle sees is that whenever you're stuck or whenever things are really frustrating just do the opposite of the thing you were doing before. So in this instance, I've lived Urban my entire adult life since I was 20, I think 20 I  Go to Philadelphia and I've lived in Philly and New York City and Los Angeles and I before that lived in suburbs and so I have never lived rural in my whole life. So I'm literally going to be living. I'm on the very edge of like the little village that my mom is in. So I'm like in the farthest corner lot. And so I'm like, wow, I'm like literally going to be like facing the forest all day long. Like what's that going to be? Like am I going to fully become Snow White will the mice and the bunny?  And the deer and the birds follow me around and I just going to sing songs to them. This is like I'm really kind of picturing. My life is some some sort of weird hybrid between like like Disney princess and a small town CW plot like, you know divorce a former lawyer moves boomerangs to her mom's retirement community lives in the woods. The closest town is like 9,000 people and  Like how maybe I'll meet a lonely widower who needs to learn about the spirit of Christmas or something. I don't know. Like I just like have to be positive about it because like optimism is the price of entry for being an entrepreneur and believing in yourself and like honestly like I would prefer to be optimistic then depressed and that's my other option and I'm not taking it because I am moving forward. I am already planning to get my seasonal affective disorder lamp. I've already started dosing myself.  Stuff with vitamin D three times a day. I'm ready to pry myself for self-care because I want to see what it's like for me to just Thrive instead of having to be in survival mode. I've been in survival mode. Like honestly like Dara sent me this shitty awful email blaming me for everything wrong in our relationship a year ago like a year and change ago. We just hit that year mark up a few days ago and I'm like, wow, my life has been like upside down like my wedding got cancelled like everything has been upside down.  For a year, I am over it. I am ready to just keep moving forward and like like I feel like in the last few months just like since she withdrew her support financially for me. I've just been like sinking into debt and like not getting ahead and I'm tired of it like I want to thrive and so it's kind of like when I moved to LA and I was like, ooh, what will my life be like when I don't have to fight seasonal affective disorder and live in New York City for six months a year because it really just felt like I was like trying to just  Just basically feel okay and instead of like thriving and then in La like I got to focus all that stuff care. I'm just thriving and I worked I love living in LA and I've really bloomed here. But I think this will be even better like having less money demands on my life and like being so in with nature, I think that I can Thrive even more and I'm excited to see what what that looks like. So stay tuned. I will keep you updated. We're going to take a quick music.  Jake and then we'll be right back. So West Coast wave goddess some were chilling on the balcony on the beach mobile some Haze.  I woke up on the island feeling Grand. I'm the queen of this Lane Highway weeks. If up the rose hip dick make my lungs expand dandelion tonic when I need to reboot. What if you're trying to find God searching for Loop close your eyes take a deep. Inhale. It's funny how they come.  / prescribe this building would like a moment in my Vortex instead of legs. I just want to heal and Thrive with my brethren. Look at how I live my life a reference. I just took a dip in the river watch me use this tofu. So, you know, my energy is lit every space that I create so much love for the kids. I'm a motherfucking guy was so much love to give my womb first entire universes my only human trillion God ancestors coming through  Reversal double back at that time PSI 3 Stacks where you're at. I've been calling it and just want to balance your chakras be your best friend. My little Reiki and I watch you turn your lifestyle up and let your light shine bright 88 some love A35. I like zzy on the worldwide. It's likely you will spend moment. I come around. That's why it sells out when I visit they tell me if the big next week.  New Mexico receipt of China and bright at Me Out World Fest way when the drive I invite you to session with me the grass is greener to kills organic feeling stressed and hell don't panic make excellent your daily Standard pick rough make like your daily and run away when you seek expansion. All I see is a mansion Community. It is my passion. I just took a shit pastor and a galactic giving gratitude I could  You passed the realm of limitations. I'm at Nippers and I'm off of average the magnet to my greatness is what keeps me free. Huh? Let's breathe. Let's win. Let's heal.  Know what she spoke with such as West Coast way goddess.  Okay, welcome back. That was Lizzie Jeff with ladybug free a recent drop. I was introduced to Lizzie Jeff by Ashley Manta who is who introduced me to Deidre Bliss who was in my last episode. So I feel like this is like all part of this beautiful Nexus of blowing goddesses. I'm really stoked about it. I love Lizzie Jeff stuff. Okay, so I wanted to talk about like the  That's of deciding to leave Los Angeles and like my perspective on it. I think that some may perceive or I guess there's a pot like there's a way to look at it where I failed to thrive in La like I didn't, you know do the thing. I couldn't support myself and I you know, I went at length and episode 2 about like what I learned from allowing myself to be supported by my partner and a lot of people have told me not to be so hard on myself.  Elf and again hashtag Capricorn problems like it's hard for me not to take personal responsibility because I really truly believe that I am the creator of my own life and I think I really needed for my heart and for my maybe even my ego. I just needed to like really give La a shot and really try to do it myself. If I was meant to be here and I tried really hard. I tried everything I could to get a roommate or job. If either of those things had come through I probably wouldn't need to leave la  Um, but it's so interesting that it neither did and I really never look at things as failures or I work hard to not look at things as failures. I work hard to perceive my failures as lessons because I think when you fail at something it is a lesson that you are doomed to repeat if you choose not to learn the lesson or even look at it or examine it and so for me, I feel like it's such a great lesson such a good learning and also I don't think that  That I failed at anything. I think this is just like I thought that you know staying in La was probably what was meant for me and like turns out that's not what was meant to happen. And I don't feel like moving down the street from my mom is like anything but an awesome next step in my life like I moving out here and getting to live so close to my grandmother for the last year of her life, which I didn't know was her last year was such an opportunity that like I knew  That my next up after Allah or even like actually had visioned for myself to live part-time near my mom and live up there. So it seems interesting that now like it's just not exactly how I decided it was going to be it's just what is happening. And what's it kind of like my path of least resistance, right? Like I didn't have enough money to pay rent and I like have to borrow the money in order to move and getting into further debt, which I don't want.  But you know, it does mean that I can like start sort of turning the ship around a lot swifter because these fences are just so much lower up there. I've also you know what I'm excited to do something different than what I've been doing right like so I get to do the opposite of what I've been doing which I think will accelerate my personal growth and I also am just like kind of excited to see what happens with my life and what's possible for me out of all of this and  And you know, I think life does get to be as easy as it can be. I think there really is a path of least resistance often that were ignoring because we're so used to things of value needing to be work and that's like a capitalist mindset where your productivity measures your worth or your results measure you're worth but I think I'm worthy no matter what I also think. You're worthy. No matter what you were the exactly as you are and I just don't feel like I have anything  To prove right? Like I'm I know no matter what I'm going to keep growing and I'm going to keep being an awesome badass and I'm going to get better at being me every year. I'm going to become a more exalted and authentic version of myself. I'm going to have more joy and more spirit because I'm continuing to step forward on my journey and learn the lessons that come my way. I heard recently that like 90% of life gets to be easy and 10% are these like  Necessary lessons we have to learn and and that's like a spiritual perspective on life and struggle and I think anything worth doing is worth fighting through resistance. And anytime you're doing something great. Like that's when resistance flares but most if you haven't read the book The War of art, I really recommend it. My mentors have me reading some books and really looking at resistance in the role of resistance in life. And I have decided there's like two areas where I'm willing to run a pill  and and that's you know, building business and personal growth, but I don't think I need to run uphill in every area of my life and it is like just the way that things kind of came together for me to move down the street from my mom like this house was available and like my mom had a storage space that she was subletting to someone else who like literally like when I needed when I had made the Declaration like I think I need to move my  My mom was like my mom's friend was like, oh we're leaving the storage space. So it's like all lining up super well and super easily and like I feel super grateful for like the work that my mom has done to help me line this stuff up, but also my mom downsized within the last few years, so she had wisdom to offer me one of my besties also downsized or like kind of combine households with her partner or fiance and now I've you know, she was like telling me on the phone the other day.  She's like, I hope this isn't super bossy and I'm like no you've literally done exactly what I'm trying to do. Please tell me how you've done this because like I think in life often, we take the opinions of strangers really seriously, like we're really concerned about what other people think I'm really only concerned about the opinions of people who have done what I want to be doing, right? So she has successfully sold a bunch of stuff from downsizing. I want that. My mom has successfully downsized her life into a place where she doesn't need a big  George unit, right so she has fruit on that treat. So I'm taking advice from those people. And anyways, I think it's just important to like have a good perspective on my life and like why I'm making the choices. I'm making I feel super happy. Actually that I had the experience of Summer of going on a full social media Hiatus. I have been producing content like personal content online like personal narrative since 2002, so  Had a Blog since then various incarnations, but like have been public on the internet since pretty much you could be and then the summer or the spring summer. I went off social media was just too became really toxic for me. It was my mental health was suffering deeply not just because of that but because of circumstances of my life C episode 2 and I needed to take a break I needed to do something different. I really really really love social media. I think social media gives us access to a  At form and it gives us access not only us as creators access to platforms. But I think social media also gives us as consumers access to mindsets and connections to people that are really powerful and can really powerfully impact our life. I think that one of the most toxic things we consume is our media TV shows commercials apps, all of these things can be incredibly toxic and can be incredibly degrading to  Our sense of self to our empowerment to who were meant to be in this world, right? And if you're spending any time at all paying attention and I use that term very specifically you are paying attention because you have two primary resources in your life time and money and when you spend your time paying attention to someone that's a real thing like that's value that you're giving with your attention, right? So you're allowing someone to have impact on you and if they are making you feel  The shitty if they are making you feel less than if you are like deeply in compare and despair and it might not even be that person's fault. It may just be your toxic interaction with that person, right? Because like you're triggered by that person and you're getting into this feedback loop step away take a like unfollow anybody that makes you feel less than unfollow. Anybody that does not help inspire you to be your most confident self and and it may not even have to do with the content. They're creating. It may just have to do with like the way you're in.  Acting with them as a person and you just need to take a break in that's okay, but there are tons of people out there who are who are really pumping out kindness and inspiration and joy, and that's why I like to ask my guests on the show who they're following on Instagram who they love because I want you to be able to have ideas of people that you can connect to who can sew into your life and who can help you sort of raise your vibration and become the best version of yourself, which is what I want for you and also PS thanks for tuning.  Me into my podcast. Thank you for paying attention to me. I really appreciate it. And I'd love sharing this stuff and it's helpful to like have people who were listening because if it's a if I'm just like recording audio on my house and don't have an audience. That's not really a podcast. Right? So what I realized I took a social media Hiatus because I saw the toxic impact on my life and I was like I need to do something else. So what I started doing with all that time because I realized so I'd love social media right? Like I love it. I only follow  Go who I love I only like I'm constantly like evaluating. Do I love this person? Are they adding value to me? Like right like so anybody who says that they feel shitty after scrolling social media too long. I mean, I think number one like so much scrolling, right? Like I don't think it's good for you but never to like follow people that Vibe you up and make you feel great, right like dipping in and out of Instagram can be something that makes you feel better about the world because you're curating your own media feed. You're not letting CBS or NBC make  Asians about what you're listening to that's that was a bygone era and now we have way more personal autonomy when it comes to what we're consuming. So instead of being on social media, I you know cuz you can't just take something out you need to replace it. So I replaced it with an hour of meditation everyday I have was on social media probably creating content for about an hour a day and then consuming content for about 3 hours a day. It was bananas at my height. And so  When I went off social media I had more time. So I was meditating an hour. I also like was working at a time. So it was helpful because I was driving Rideshare so much. I was basically just talking to real people in real life and making real connections with them rather than like just having the social media thing which like I think social media is good for Content similar to reading a book or watching a television show. I do not think it's good for social and human connection. I think human connection actually requires a connection.  AKA get on the phone with people. I was also replacing it with phone calls. I have talked to so many of my friends on the phone and the last few months. It's amazing. I can tell a significant difference between my mood on a day when I've talked to three to four to five people on the phone then just 0 to 2. There's death. I just like that human interaction that human connection cannot be duplicated. I think be a digital means it's not texting. It's not sending DMC on Instagram it is  Only talking on the telephone or having a video chat conversation with someone. I've all the eyeball voice The Voice something like that. That isn't meaningful human connection humans are a tribal species. That means that we thrive on social connection. It's actually really vital to us. And when I was on my social media Hiatus, I had a friend send me an article that said social isolation kills and I was just like wow that's deep and hard but I am definitely still getting my social interaction just in a  Different more low-key way and and actually, you know what? It was really fulfilling like not being on social media like and having those human connections kind of replace these like fo connections that I thought were real human connections was like eye-opening in terms of just like how humans Thrive I have really been challenging myself to talk to strangers more and more, you know, kind of beefed up by the great conversations and connections. I had through lift, but also like just  Maybe some sunshine and people's lives like sometimes when you talk to someone at the grocery store you maybe be the only person that person is talk to you today. And I think it's awesome to just like give someone a compliment. I think Compliments are a free thing. You can do that make people feel awesome and I highly recommend complimenting on things that are non physical appearance related things like oh I like your shirt. Where did you get it right like something really specific about what you like about someone about how they are.  In the world or like, you know, I don't know whatever talk to strangers. It's not going to it's nobody ever died of awkward and like if I view up but that's the long and the short of that but I really started just like challenging myself to do new social things and that was really good and it was harder for me to like see my friends because I was working so much mostly like when I was working for left, I was like doing self care and working and that was it because like I had a better driving day if I'm  Agitated and I definitely never would feel comfortable driving people in a car unless I had a full night's sleep. So those are two like primary huge self-care things that they needed to do. So something I also did while I was on my social media Hiatus is I created a second Instagram account. That was just like a secret account that I used to consume just a very small number. I'm talking like 5 to 10 people on social media and the reason why I  Did that it's a weirdest thing. I started missing John Mayer. So John Mayer grammy-winning a recording artist. You probably have heard his stuff. He's very famous. He I got introduced to him because of Andy Cohen who's like the Bravo host the executive producer of The Real Housewives and Andy. I don't know. I just started watching Watch What Happens Live? I really liked him read his book superficial and I I just like really  Next John Mayer just based on how Andy talked about him. So then I started falling on him on social media and I actually like really deeply admire him as an artist like he has an amazing art practice. Like he's a brilliant guitar player really good songwriter and like deeply connected to emotions. I've also looked up his birth chart. So his Mars is in cancer if that means anything to you, but that means that your work in the world is about feelings. So Mars is your work.  what the sign of cancer is all about feels and so anyway, so he just has a really good Lyricist way of connecting to feelings and I yeah, I also like he's I think studied comedy or at least like has at least been a student of Comedy because he's very funny and like you can tell like he's like working in refining on that and any artist who learns like it's basically like an art hack right like a Mastery hack like when you've got an area of Mastery and you  Start exploring other artistic Expressions outside of that mean expression it it actually makes the main expression stronger. I think it's something to do with brain science. I read it in a book and so like anyway, and also if he's funny and he makes me laugh out loud and I specifically need more people in my life who make me laugh out loud, like I'm 40, like I don't have time to have a boring life anymore. Like I want mirth enjoyment and dad Joe.  Jokes, you know what I mean and like so I'm just I'm here for you. I'm here for people who make me laugh. And so then I just I enjoyed follow him on social media because he regularly makes me laugh out loud because I admire his artistic practice and enjoy a consuming his art. Like I'm now a big fan like it's funny because like his social media got me listening to his music more now, I love it and I just like found myself like in like month one of my social media Hiatus curious have John Mayer  You know what? I mean, and I'm like wow, this is interesting of all the people I follow on social media and it's like more than a thousand on my main account which is at Bevins party. If you don't follow me yet. I was like Wow John Mayer, huh? So like I just I'm also an artist who follows her Curiosities. I think Curiosities are the way God sort of leads us to things and so I decided I was like, okay if I'm curious about John Mayer, why don't I like you know, like  Go see what he's up to. So I logged into like my side account and just like followed him followed like a couple other entrepreneurs who like have what I want and like I said like you take advice from people who have the fruit on the tree that you want. And so I followed my leek teal who is an amazing boss CEO and just really good and resilient about what other people think she  Preaches a lot about being patient with the process and really just like her podcast is also Top Notch really recommend a follow to that. I also followed Gary Vee who is this prolific content creator. Once you see the amount of content, he has that every time I'm like, oh my God, he's putting out so much content. I remind myself. He is a 30-person Content creation team, which means he's like not effing around about creating content, but he preaches optimism and patients all the time which are  very crucial characteristics of any entrepreneur and I needed those reminders because like, you know, you don't have any control over like the market and how things go you only have control over your work product and it is your work product is very very dependent on your perspective and I really needed people to remind me to be optimistic and patient. And so I started following them. I started following like a couple of my close friends who have babies.  Is because like they're like minutes pass and they get bigger like it is so bananas when you like have kids in your life who are growing. I love being an aunt and I love connecting two kids and I really appreciate my friends who share their babes on social media so that I can see them grow up like kind of minute by minute. It's yeah, it's just like something I was really missing like seeing my little nieces and nephews and so I was like, I want to see them so I started end.  It up. I ended up like following like maybe 10 people but the best part about it was that my feed would end like you can scroll through and it's done. So if you only follow 10 people and I'm not saying like you have to get a whole new account for this you could dump everybody on yours and only follow 10 people, but I think it's super potent to be consuming everything someone's pumping out like because that's a person who has what you want and that's why you're connecting with them. And so it was great because like 15 minutes and that was it. So I was  And out in 15 minutes. It did not drag me down. It actually Amplified me and helped me feel better in life. And but also not consumed by her connected to social media too much and now that I'm back on social media. I oh and also one more thing I wanted to say about that. It's so you are truly influenced by The Five People You spend the most time with it really shapes who you are and I really curate who those people are in my life.  Life as soon as Darren I broke up. I worked really hard for her to drop out of that top five because she is so fear-based and like I knew that how much fear she had and like scarcity mentality. She had I knew that was affecting me and I knew it even while we were together, but I just loved her too much and I was too committed to our relationship to like critique that or try to take her out of my top five, right? I wouldn't marry someone who I wouldn't want my top 5, but now I see it and I'm like, oh  I like just needed to get that negativity out of my head and I needed to get that like worry some sort of stress ball stuff away from me. So I worked really hard to like curate my top 5 and who's influencing me and I think you can do that with people who you were not directly like in a relationship with right like they don't have they can be your friends. Like if you are like sitting around with one of your besties all the time, or like if you're a parent and you have  Current friends who are like friends of your kids that you don't really like that much but you're spending time with them because your kids are together like those people are affecting you but I think that if you put some time into it and intention into it, you can be influenced in a similar Way by people who are content creators or artists or whatever. I really got into consuming everything that one person would put out. Like if someone had a podcast I might just listen to that whole podcast or listen to them regularly.  Like every week so so Kyle sees is someone I very intentionally making my top five because he has what I want. He's happy. He's fulfilled. He's living a life of impact and he's financially I wouldn't call him financially independent, but he's pretty close like and so yeah, so like being part of his absolutely everything past community and being on his weekly calls, like that's just a way that he is like sowing into me his beliefs and perspective and allowing me to kind of  absorb that and and put it into practice in my life. And so yeah and like also like artists who you if you're an artist and you don't have artist whose practices you respect that you are trying to have in your top five. Like I you're not going to necessarily grow as an artist like you need people who are helping you grow if you want to grow in life, and if you don't want to grow in life life is just going to keep kicking your ass until you decide that growth is what you want from.  Actually, that's it's just my my wisdom and like, you know, sometimes you're not even noticing the life ask his ass kicking because you're just used to it like this is just how life is sometimes life kicks you in the ass most of the time this is where that 90/10 thing comes in. I really truly believe and this is Bevin having lived through a year where it was not 90/10. It does not have not had 90% He's and ten percent resistance. It has been kind of more of the other way around but you know, they in spiritual communities they call it.  The Dark Night of the Soul when you're really like having to grow through some stuff and I've had to grow through some stuff but I'm grateful to feel like it's on the other side because like just choose to believe that I don't have I don't have anything that I got a lot of packing I still gotta do get a lot of decluttering to do but I'm just believing for it. I'm trying to just be in that mindset that everything is happening for a reason and that I am you know  Be less depressed because I am focusing on human connection having at least like five real human connections every day. And yeah, like really just and that's going to be a challenge living rurally and like I'm glad that I had the practice of a social media Hiatus so that I can know what it's like for me to like change radically how we interact with humans and also just have that data about me that having five kids.  Connections in a day really does help me stay a little antidepressant right? And you know, since I've been back on social media, I think I've been back for like a month and a half. I still mostly consumed. My phaedo cardi B also isn't made it into those 10 people because I love cardi B. She's an amazing artist. I really loved her perspective in life and she teaches a lot about how to be resilient to haters. So anyway, I have still consumed.  Feed but I'm also still can I'm also consuming on my main feed again and I just love social media. It's hard for me not to do it. And so I have to really like monitor my time and not get stuck in the black hole of it. But you know, I'm chatting. I like it anyway, so I hope any of this helps like I hope that any of my experience is resonating with you and that you are out there thriving yourself and out there figuring out where your next area for growth is.  And you know I share this stuff because it's helpful for me to hear other people's experience and I frankly want to double dip. I will if I might find one I have to go through some shit and hurt and pain and resistance and all of that. I want the double dip of not only learning from it, but sharing it and like having this be for more than just like the pain I had to experience. I really want to help other people experience a glow up and have a great life. I'm really  It's like when it comes to social media and stuff like as a quote-unquote influencer, I'm really not interested in followers. What I'm interested in is leaders people who are out there who want to lead you have to start by Leading yourself, right? Like so who want to lead in their lives, but then also like leave whether or not it's like in a formal capacity by taking on leadership roles or if it's just informally like in your life providing an example of someone who does Growth work grows in life is more kind trying.  To do better things in the world, you know, like you just don't know who you have an influence on who you have an impact on in your life and I frankly don't think follower counts have anything to do with worthiness. I think in fact, sometimes they can mask a lot of things but I'm really out here just like looking for leaders people who like want me to help incubate their self-care, you know, that's a that's my ultimate goal for my patreon is like to have leaders who, you know want to have a movement practice with me. I think it's super meaningful.  The shop for your body and for yourself at least once a week to like have some intentional movement practice and I always try to like download nuggets and I think it's really powerful to have, you know, lessons and bigger philosophies coming into movement class other than like having to get some aspirational body. Right? Like I'm here for aspirational like life and empowerment and fun and enjoy an impact.  So anyway, that's my philosophy about that. I hope that wherever you are, you know that you are worthy of Love exactly as you are. You're amazing. You are an inspiration to many and you have no idea or maybe you do and I just like really appreciate you tuning in. I really appreciate you, you know being here wanting more for your life and and just like take a little time to dream like take a little time to think about like what's on your heart.  What's your next big thing? And like why do you want to make an impact on the world? Just think about those things? I think so few people even spend time dreaming up what their life can be like and you're never going to get there. If you don't even think of it, you know what I mean? Like, I think like a lot of us just kind of like let life, you know sort of push us around and like end up places without really like consciously like considering like what can it look like or what what's possible for?  Or me and like the more time every day that I spend just like in the possibilities that's available the more I'm able to connect into. Oh, that's so that's kind of a calling or that's something that's really speaking to me or oh, I love that idea for later. Maybe but maybe not. I'm not going to work on that right now, right? Whatever. Anyway spend some time dreaming. I've been signing off for forever. I'm going to cut this off, but I want to like play out with little John Mayer his most recent at-bat Carry Me Away. I  I love an artist whose favorite like who's just constantly turning out something amazing and like it's his latest song is so far my favorite John Mayer song. So you're only as good as your last at-bat. I learned that from Gary V. And here's John Mayer's latest at bat. Love you all. I'm such a boy. I'm such a girl.  Bummer there must be more behind the summer. I want someone to make some trouble been with you say inside my bubble take me out and keep me up on that.  Carry me. Carry me.  Scanning can I have some more? I can't understand it you fast car you foolish spender, you know you are and I Surrender come on and wake me up. That's kind of crazy.  Carry Me\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx34IdjPvb_5",
        "colab_type": "text"
      },
      "source": [
        "#First N and t5 w/ coreference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PN-fUg4xBPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en')\n",
        "neuralcoref.add_to_pipe(nlp)\n",
        "\n",
        "def coref_resolution(transcript):\n",
        "  doc = nlp(transcript)\n",
        "  return doc._.coref_resolved"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-maND-uyl2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def firstn_t5_coref(transcript, limit, threshold):\n",
        "  sentences = first_n(transcript, limit)\n",
        "  resolved_sentences = coref_resolution(sentences)\n",
        "  summary = t5_inference(resolved_sentences, threshold)\n",
        "  return summary"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2p2Ap5gzT7d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "800cf34e-04df-4155-da6f-50669047bfef"
      },
      "source": [
        "ext_abs['firstn_coref_summary'] = ext_abs.apply(lambda row: firstn_t5_coref(row['transcript'], 15, 7000), axis = 1)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (601 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjcZ0MSJ12EG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed18a69e-d0ef-450b-eac5-ecac9963c9dc"
      },
      "source": [
        "rouge = Rouge()\n",
        "rouge_scores = rouge.get_scores(ext_abs['firstn_coref_summary'], ext_abs['episode_description'], avg=True)\n",
        "rouge_scores['rouge-1']['f'], rouge_scores['rouge-2']['f'], rouge_scores['rouge-l']['f']"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.19460879330911204, 0.0948191636225971, 0.2267655938612496)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzVTfi9dHdSx",
        "colab_type": "text"
      },
      "source": [
        "Pretty much the same as without coref, only marginally better. So this says that we definitely do not need most of the transcript, and coref is a nice to have"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlUcq9kyCe8I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "6c43a316-a752-4b5a-ac8f-d245dd45e828"
      },
      "source": [
        "ext_abs.iloc[10]['firstn_coref_summary']"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This week, Nick and Jeff discuss college football news on the College Football News Podcast. --- This episode is sponsored by  Anchor: The easiest way to make a podcast. https://anchor.fm/app Support this podcast: https://anchor.fm/collegefootballnews/support this podcast: https://anchor.fm/collegefootballnews/support this podcast: https://anchor.fm/collegefootballnews/support this podcast: https://anchor.fm/collegefootballnews/support this podcast: https://anchor.fm/collegefootballnews/support this podcast: https://anchor.fm/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3raHIjEACiEt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "d3756b6f-6071-4f16-f44f-59b7e7bb0920"
      },
      "source": [
        "ext_abs.iloc[4]['firstn_coref_summary']"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"This is the first episode of The Other School Podcast. I talk about what it means to be a teenager and why you should keep listening! Thank you so much for your patience, thank you so much for tuning in and thanks for listening! Today's session is going to be a bit different from the norm. We will go over what they don't teach you at school and how to learn these essential lessons. From teenage Finance earning money to personality Improvement. Please leave us a review on Instagram or Twitter @UncleSharif Sharma Facebook Group: https://www.instagram.com/uncleSharif Sharma YouTube Channel: https://www.youtube.com/uncleSharif Sharma\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nSjzqw2CnQe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "b33f4c66-6226-4e86-9840-e2e38e7bd275"
      },
      "source": [
        "ext_abs.iloc[4]['episode_description']"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Introduction | Show Formalities | What to Expect This show is all about real learning.Teaching youn what they dont teach you at school. Everything you struggle with as a teenager.And the basics of what should be taught at school.for more information email @unschoolpodcast@gmail.com Host ; Ankush Sharma\\xa0 '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-z5xkvBGrRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}