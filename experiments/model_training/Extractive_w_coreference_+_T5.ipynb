{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extractive w/ coreference + T5.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMbS7sPh_ErP",
        "colab_type": "text"
      },
      "source": [
        "T5 and Bart did well. So did Bert + Kmeans with coreference. What happens when we use both."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dur-bDxr_hlM",
        "colab_type": "text"
      },
      "source": [
        "#Standard installation and imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHNKija7CTAo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d0c77242-30d4-4585-f367-fe5abea66be2"
      },
      "source": [
        "#set up again because restarted runtime\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "project_id = 'test-281700'\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "#download model..restarted runtime\n",
        "bucket_name = 'spotify_asr_dataset'\n",
        "!gsutil -m cp -r gs://{bucket_name}/t5-model-3000.zip /content/\n",
        "!unzip t5-model-3000.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "Copying gs://spotify_asr_dataset/t5-model-3000.zip...\n",
            "| [1/1 files][787.9 MiB/787.9 MiB] 100% Done  39.6 MiB/s ETA 00:00:00           \n",
            "Operation completed over 1 objects/787.9 MiB.                                    \n",
            "Archive:  t5-model-3000.zip\n",
            "   creating: t5-model-3000/\n",
            "  inflating: t5-model-3000/pytorch_model.bin  \n",
            " extracting: t5-model-3000/tokenizer_config.json  \n",
            "  inflating: t5-model-3000/config.json  \n",
            "  inflating: t5-model-3000/spiece.model  \n",
            "  inflating: t5-model-3000/special_tokens_map.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG5uCvQouwOQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "16868313-fa5b-4ce6-c0c2-edb9d9198fa8"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install rouge\n",
        "!pip install neuralcoref==4.0\n",
        "!pip install spacy==2.1.0 #notebook crashes on other versions\n",
        "!python -m spacy download en\n",
        "\n",
        "!git clone https://github.com/paulowoicho/bert-extractive-summarizer.git\n",
        "!mv bert-extractive-summarizer summarizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 15.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 17.9MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 41.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=6a180310e497074d540cd3e215cccf1dd0b768906ad81e22522910dccca0d22e\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n",
            "Collecting neuralcoref==4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/24/0ec7845a5b73b637aa691ff4d1b9b48f3a0f3369f4002a59ffd7a7462fdb/neuralcoref-4.0-cp36-cp36m-manylinux1_x86_64.whl (287kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref==4.0) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from neuralcoref==4.0) (1.14.30)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref==4.0) (1.18.5)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref==4.0) (2.2.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2020.6.20)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->neuralcoref==4.0) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.30 in /usr/local/lib/python3.6/dist-packages (from boto3->neuralcoref==4.0) (1.17.30)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->neuralcoref==4.0) (0.10.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (0.7.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (49.2.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (3.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (1.1.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.30->boto3->neuralcoref==4.0) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.30->boto3->neuralcoref==4.0) (2.8.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.1.0->neuralcoref==4.0) (1.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.30->boto3->neuralcoref==4.0) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.1.0->neuralcoref==4.0) (3.1.0)\n",
            "Installing collected packages: neuralcoref\n",
            "Successfully installed neuralcoref-4.0\n",
            "Collecting spacy==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/39/4bde5da5f18ab0bdd525760c4fe38808b4bb03907a2aea094000d831afe1/spacy-2.1.0-cp36-cp36m-manylinux1_x86_64.whl (27.7MB)\n",
            "\u001b[K     |████████████████████████████████| 27.7MB 111kB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (0.7.1)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 38.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (2.6.0)\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.1MB/s \n",
            "\u001b[?25hCollecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (2.23.0)\n",
            "Collecting thinc<7.1.0,>=7.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/a5/9ace20422e7bb1bdcad31832ea85c52a09900cd4a7ce711246bfb92206ba/thinc-7.0.8-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 34.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (1.18.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (2.0.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.0) (4.41.1)\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: blis, preshed, plac, thinc, spacy\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: preshed 3.0.2\n",
            "    Uninstalling preshed-3.0.2:\n",
            "      Successfully uninstalled preshed-3.0.2\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed blis-0.2.4 plac-0.9.6 preshed-2.0.1 spacy-2.1.0 thinc-7.0.8\n",
            "Collecting en_core_web_sm==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1MB 1.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.1.0-cp36-none-any.whl size=11074435 sha256=d60d53e9586d342cb4206dd5a3c4a14da36aee462615b95c6cc5b86e676b1888\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_798v7mu/wheels/39/ea/3b/507f7df78be8631a7a3d7090962194cf55bc1158572c0be77f\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Cloning into 'bert-extractive-summarizer'...\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 378 (delta 0), reused 3 (delta 0), pack-reused 374\u001b[K\n",
            "Receiving objects: 100% (378/378), 85.48 KiB | 2.95 MiB/s, done.\n",
            "Resolving deltas: 100% (215/215), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6htGWio_4JW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e19c275e-a1bf-4255-b62f-a423939fccf2"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from summarizer import Summarizer\n",
        "from summarizer.coreference_handler import CoreferenceHandler\n",
        "from transformers import *\n",
        "from rouge import Rouge\n",
        "import spacy\n",
        "import neuralcoref"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 40155833/40155833 [00:00<00:00, 43455822.07B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1ixRR7FA-75",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fc300ede-fcb1-4578-b78e-4403f87a29fd"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvgcWuUNAanf",
        "colab_type": "text"
      },
      "source": [
        "#Spanbert + t5 pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV3zThUjBRRc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "0573c511-17dc-4c50-9c02-191d84ec3b3c"
      },
      "source": [
        "#Spanbert\n",
        "handler = CoreferenceHandler(greedyness=.4)\n",
        "custom_config = AutoConfig.from_pretrained(\"SpanBERT/spanbert-base-cased\")\n",
        "custom_config.output_hidden_states=True\n",
        "custom_tokenizer = AutoTokenizer.from_pretrained(\"SpanBERT/spanbert-base-cased\")\n",
        "custom_model = AutoModel.from_pretrained(\"SpanBERT/spanbert-base-cased\", config=custom_config)\n",
        "\n",
        "spanbert_model = Summarizer(custom_model = custom_model, sentence_handler=handler, custom_tokenizer=custom_tokenizer)\n",
        "\n",
        "def span_bert(transcript, ratio):\n",
        "  result = spanbert_model(transcript, min_length=60, ratio=ratio)\n",
        "  return ''.join(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:transformers.modeling_utils:Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-base-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9UN_WBgAqPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#t5\n",
        "tokenizer = T5Tokenizer.from_pretrained('/content/t5-model-3000')\n",
        "model = T5ForConditionalGeneration.from_pretrained('/content/t5-model-3000')\n",
        "\n",
        "\n",
        "def t5_inference(transcript, threshold):\n",
        "  t5_form = 'summarize: ' + transcript\n",
        "  tokenized_text = tokenizer.encode(t5_form, return_tensors=\"pt\")\n",
        "  if len(tokenized_text[0]) > threshold:\n",
        "    #run out of RAM/crashes on large number of tokens\n",
        "    revised_text = sent_tokenize(t5_form)\n",
        "    length = len(revised_text)\n",
        "    final_text = revised_text[:int(length/2)] #maybe they talk about content in the first half? find proof\n",
        "    text = ' '.join(final_text)\n",
        "    return t5_inference(text)\n",
        "  summary_ids = model.generate(tokenized_text, max_length=150, num_beams=2, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\n",
        "  output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW6CwOYTBu2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def spanbert_t5(transcript, ratio, threshold):\n",
        "  important_sentences = span_bert(transcript, ratio)\n",
        "  summary = t5_inference(important_sentences, threshold)\n",
        "  return summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNLC73QSCu3o",
        "colab_type": "text"
      },
      "source": [
        "#Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZISZgaXCK82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "reference_results = pd.read_csv('150_gold.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qwayo6DcC8oN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "2080fa01-1772-40f8-8a55-3ae61fe38a16"
      },
      "source": [
        "ext_abs = reference_results[['episode_id', 'transcript', 'episode_description']]\n",
        "ext_abs.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>episode_id</th>\n",
              "      <th>transcript</th>\n",
              "      <th>episode_description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>spotify:episode:08hXUWN6aOnHULXrqMiwTi</td>\n",
              "      <td>Hello everybody. What's going on in this Jess...</td>\n",
              "      <td>If you want to start mastering recruiting whic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>spotify:episode:0CExTNH4LFqp1ec1mhTd4I</td>\n",
              "      <td>Ladies, have you been molested don't be a vic...</td>\n",
              "      <td>Don't be a silent victim of crime. a parody fr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               episode_id  ...                                episode_description\n",
              "0  spotify:episode:08hXUWN6aOnHULXrqMiwTi  ...  If you want to start mastering recruiting whic...\n",
              "1  spotify:episode:0CExTNH4LFqp1ec1mhTd4I  ...  Don't be a silent victim of crime. a parody fr...\n",
              "\n",
              "[2 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0FL5258DSWj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "779f4418-dc9f-431b-d84b-3c63b190de77"
      },
      "source": [
        "ext_abs['auto_summary'] = ext_abs.apply(lambda row: spanbert_t5(row['transcript'], 0.1, 7000), axis = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (853 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (697 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (800 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (781 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (881 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (1144 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (774 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (758 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (744 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (578 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (692 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (650 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (735 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (766 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (939 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (971 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (670 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (783 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (649 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (697 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (694 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (672 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (1421 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (897 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (920 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (927 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (723 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (603 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (1035 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (1117 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (894 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (1143 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (696 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (702 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (1022 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (763 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (1196 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (724 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (797 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (770 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (1093 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (779 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (876 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (1004 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (695 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (839 > 512). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcWAQpenwXiF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "7ed6ef64-b69b-4290-a799-89490526e9c8"
      },
      "source": [
        "ext_abs.iloc[1]['auto_summary']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ladies, have you been molested don’t be a victim of crime here are some things you can do when it’s late. Try your best not to be molested by a culprit from a good University who has good academic results and the potential to excel in life. When it’s late try your best not to be molested by a culprit from a good University who has good academic results and the potential to excel in life. If you’ve been molested, try your best not to be'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT0-V5IWw-BL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c53865a4-20db-4097-9e0f-c0eca495bac9"
      },
      "source": [
        "ext_abs.iloc[1]['episode_description']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Don't be a silent victim of crime. a parody from mrbrown.com \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFu8vgJrEAb5",
        "colab_type": "text"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkjkwEyOv28D",
        "colab_type": "text"
      },
      "source": [
        "##spanbert + t5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpgNDCV5GSwl",
        "colab_type": "text"
      },
      "source": [
        "Just realized I initially did not use Rouge API properly. So I may need to recompute the other results I had gathered, depending on these following results :(. On the other hand, it just means recall and precision scores are switched, so f measure should be the same"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_QUApEnENTh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1dede8b4-c67c-4e95-b97a-9757ce835057"
      },
      "source": [
        "#what i have been doing\n",
        "rouge = Rouge()\n",
        "rouge_scores_w = rouge.get_scores(ext_abs['episode_description'], ext_abs['auto_summary'], avg=True)\n",
        "rouge_scores_w['rouge-1']['f'], rouge_scores_w['rouge-2']['f'], rouge_scores_w['rouge-l']['f']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.16932254852889728, 0.0723673729650572, 0.1957123945829847)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2820LJsE7OM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a83cbaa4-f355-4215-81d0-16e01e1324da"
      },
      "source": [
        "#correct version\n",
        "rouge_scores_r = rouge.get_scores(ext_abs['auto_summary'], ext_abs['episode_description'], avg=True)\n",
        "rouge_scores_r['rouge-1']['f'], rouge_scores_r['rouge-2']['f'], rouge_scores_r['rouge-l']['f']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.16932254852889728, 0.0723673729650572, 0.19618320623105837)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "933W-DMkRb0E",
        "colab_type": "text"
      },
      "source": [
        "All is well :). This is better than all extractive techniques but worse than supervised BART and T5. It is better than semi-supervised bart though. However, this depends on the hyperparameter I used (0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yKFa9wtvUjC",
        "colab_type": "text"
      },
      "source": [
        "#First N and t5 no coreference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXG6XltOwM-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Forgot first-N again. Rather than rewriting functions, I could make it so that is is a super function that takes\n",
        "#other functions as parameters\n",
        "\n",
        "def first_n(transcript, threshold):\n",
        "  sentences = sent_tokenize(transcript)\n",
        "  return ' '.join(sentences[:threshold])\n",
        "\n",
        "def firstn_t5(transcript, limit, threshold):\n",
        "  sentences = first_n(transcript, limit)\n",
        "  summary = t5_inference(sentences, threshold)\n",
        "  return summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QELxYbO03VE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "f7e6bc9e-dfe2-4d94-e98d-1d57d6e95a80"
      },
      "source": [
        "ext_abs['firstn_summary'] = ext_abs.apply(lambda row: firstn_t5(row['transcript'], 15, 7000), axis = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAdbLjnW1Unm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db7484dc-c841-4346-e0d6-c30faa335c66"
      },
      "source": [
        "rouge = Rouge()\n",
        "rouge_scores = rouge.get_scores(ext_abs['firstn_summary'], ext_abs['episode_description'], avg=True)\n",
        "rouge_scores['rouge-1']['f'], rouge_scores['rouge-2']['f'], rouge_scores['rouge-l']['f']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.1909758054021582, 0.0962574068499044, 0.2229754751606156)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6n3EecIt10T",
        "colab_type": "text"
      },
      "source": [
        "Very surprising results! As good as t5 alone! Although no coreference was used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a3pyCtCzXz_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "2f33528a-5bee-4ec3-d3c6-722e5d404031"
      },
      "source": [
        "ext_abs.iloc[10]['firstn_summary']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Nick and Jeff discuss college football news on the college football news podcast. --- This episode is sponsored by  Anchor: The easiest way to make a podcast. https://anchor.fm/app Support this podcast: https://anchor.fm/collegefootballnews/support this podcast: https://anchor.fm/collegefootballnews/support this podcast: https://anchor.fm/collegefootballnews/support this podcast: https://anchor.fm/collegefootballnews/support this podcast: https://anchor.fm/collegefootballnews/support this podcast: https://anchor.fm/collegefoot'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48ArnuUszzsF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "b63a3f21-96f4-42de-ab6a-efe279033ba8"
      },
      "source": [
        "ext_abs.iloc[10]['episode_description']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\xa0 Nick Shepkowski sits down with Jeff Feyerer of College Football News, Inside the Pylon and various other football outlets to preview the 2019 season. \\xa0 Clemson/Alabama round 5 - who has the upper hand entering 2019? How does the Big Ten stack up to the SEC? \\xa0Who are a few underdog picks that could surprise and win their conferences? Predictions, opinions and in depth preview of what the 2019 college football season may bring. \\xa0Enjoy!  ---   This episode is sponsored by  · Anchor: The easiest way to make a podcast.  https://anchor.fm/app  Support this podcast: https://anchor.fm/cfnpodcast/support'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b13x0JS60g4H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "93e85db2-6992-44a9-fc16-14117c244827"
      },
      "source": [
        "ext_abs.iloc[10]['transcript']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Hey, it's Nick shop Kowski from the college football news podcast here to tell you about anchor in why I switched to it. One reason. Well, it's free. But beyond that it's awesome had trouble getting my podcast on a previous distributor distributed through, you know, the likes of apple and Spotify and some of the others anchor did that very easily without much question whatsoever. They have creation tools that allow you to record and edit your podcast right from your phone or your computer you can make From your podcast with no minimum listenership. It's everything you need to make a podcast in one place download the free anchor app or go to Anchor dot f m-- to get started.  Welcome back in another edition of the college football news podcast mix-up pow ski with you. Keep you check out for this one, but I welcome it now from a man. You've read his work you've seen as by lines on college football news Pro Football Weekly various different places inside the pile on pile on you podcast. You've heard him host before Jeff are kind enough to join me for a few minutes tonight on the college football news podcast. What's up, Jeff? Nick, man. Good talk. Good to talk to you.  College football is within our within our sights. Yeah. I know it. It's like you wait all offseason. You wait all summer. And finally it's like alright, the day count is down to single digits were about to Miami or about to Florida have that to kick things off and be the be our counterpart for the rest of the summer into the fall, but I guess the the conversation starts when you bring up college football anymore and it's like the NBA was four years with the Warriors in the Cavs is it is simple to you to the numbers play out from everything.  You've seen that it's Clemson, it's Bama or it's Bama. It's Clemson. It's a gap than Georgia and a gap and everyone else my first reaction this season following especially Following last season was I'm ready for Alabama to run roughshod over everyone. I think it's going to be a revenge tour that we've never seen before not that they need more motivation other than just having really good players. I think being being handled on the national stage as they were last year my number.  Something looking looking out. This year is is is Bama is not it's not will Clemson rebound. Are they the new Alabama but I think Alabama is ready to just run through everybody Clemson is typically lost. I think last year was unique that through the Brent venables the era they've typically lost a lot of defensive players last year. They returned a lot for the really the first time. I think they've gone a few years where they lost at least half of the guys in their defense and still being good. So this year, I mean, I'm still  expecting that but their top players all across the board now, I mean, you lost your dad we talked about feral and Wilkins and Mitch Hyatt a lot of leadership on that Squad and you know, Trevor Lawrence probably the best quarterback in the country, but where is that other piece going to come from for Clemson? So that's how I look at it right now, especially with those two teams. I do think there are a number of teams down below them that are talented enough to have it to me to have a shot, but it would be really tough.  I was putting any money on either and any team this year to go against either Clemson our Bama. Yeah, look at it in Clemson. I do have a little bit of concern just because how much of that defensive line. You've seen stars go year after year after year with them, but just the this year. It's like well, it's a collective whole holy cow. Like that's a ton of NFL Talent. They lose. I just don't look at their schedule and who's supposed to scare me? Like I honestly do not have an answer on that like who is Spike Texas A&M is an interesting game early on but there's no reason they  A shouldn't just run rampant over the rest of the ACC South. Carolina's not going to give them a scare. It's gonna be a team that's 13 and oh again, come college football playoff. And then it's them against Bama them against Georgia them against whoever and there's no reason or not one of the two or three best teams in the country. I mean the Georgia Tech opening game is going to be pretty pretty rough for for the Yellowjackets just because they're changing schemes, but I think I am I'm you saw a tough and I'm played them last year at home granted. It was at it was a tan.  But you saw a tough. They played them last year. I think this am team will be better than they were last year. This comes into your baby a little were so catching them early and you go at Syracuse the next week and two years ago. They lost in the dome and the Carrier Dome. So that's a tough early game, especially with the experience Syracuse team coming back. Yeah me the rest of the schedule. Isn't that rough? It's really those those games early on because the rest of the time they get BC at home. I like BC but getting them at home. I don't think that's gonna be a tough one, Florida State at home again.  Florida State Should bounce back and play better, but I don't see them giving us here. So it's really those the game 2 and Game 3 and passed that it really goes to is Clemson could be tested enough during the regular season to be ready to play those tougher teams and leagues is the playoff this year.  Get to the playoffs this year. I'm really saying when yeah, I mean, I think it's pretty much to me. It's a when it's a game of one like I don't see like Syracuse to change the quarterback. I hate I'm not as thrilled with Syracuse this year. I was surprised when they're first number came out. It was like a five and a half wind totally got bed all the way up to like eight eight and a half. I'm not as big on Syracuse this year that Texas A&M game I think is going to be one of the better games this season, especially non-conference games for the year, but I just don't see something in the ACC. It's Clemson. It's a huge gap and it's a lot of question marks.  After that, one of the things I think is interesting though is looking at the rest of the SEC and the talent obviously speaks for itself. Like how close for you is LSU to Bama or Florida to Georgia because I think it's it's almost like you view that conference right now is the rest of college football views Clemson versus Bama. It's a countdown until it's Bama in Georgia in Atlanta. I mean, how real is a threat from Florida? How real is a threat from anyone else in the West for?  For Alabama this year. Yeah, fuck bam in the Tier 1 by themselves, but I would group LSU Georgia and Florida together in that next tier. I mean Florida's offensive line is it might be an issue LSU's defense is going to be a killer. I mean that's that's a squad right there with and with Delta with Fulton with morons where you're going to have a ton of guys that are going to see on Sundays that are going to be a problem. It's just whether or not Joe burrow can continue what he did last year and that's always been the question with Alice. You are going to get a play making out of the quarterback position.  Offense but I do I do like LSU a lot this year Georgia. I mean Jacob from has got to play a little better. They're going to have they're going to have a good offensive line. They're going to be under a Swift and Beatrice Robinson. And then that defense are always gonna have guys coming out with a recruiting classes that they've been putting out and the Florida. I mean Philippe Franks has got to play the way he's supposed to play. I think the way they expect him to play in Florida. So I would put those teams on one tier and I know off are we talked a little bit of Mississippi State and I was  as you see some of the numbers behind them this year and we know another year or more heads offense Kyle and Hills a playmaker for them did lose a lot in Abram and Montez sweat and Jeff Simmons on defense, but I do think Mississippi state is another squad on that maybe a third tier with am right now and I am going to be coming up. I mean am look really good on the national stage a few times last year and I think Jimbo's going to have them playing. Well, I'm it's hard to look at Bama in the same group as any of these guys right now, but I do think there are a number of teams in the SEC that  People scare this year. Yeah the Ingham thing. It's you look at their schedule and it's Clemson ridiculous. And then there yeah, Texas State and Lamar out of conference, but they're at LSU there at Georgia this year, Mississippi State on the schedule Auburn on the schedule. It's like there's no slouches for them. That's got to be amongst. If not the toughest schedule for any team in the entire country. This season is going to be tough again to Auburn. This is not and I guess this is kind of the not to not to draw comparisons across  Sports but you know, how is old gets his team ready in the regular season for for the tournament. I mean this is their ready every year for the playoffs because they do play a tough schedule. They've got a cupcake sprinkled here or there but they didn't shy away two years ago and they play Florida stay in the nest in the first game of the season on a neutral field of said, let's go they roll the ball out and they took care of business. So that's bad is never a concern with me for dammit. I never look at scheduled bam. I look at the squad they have coming back. And right now you're looking at them having the probably the  Most talented quarterback date had under saving the most talented most talented receiving Corps. They've had their to save and another awesome running back and Nagi Harris who finally get his share of the carries and then a ton of guys on defense. So bam is going to be trouble and schedule really doesn't matter. You mentioned the quarterback to Otunga Viola. I always have arguments with Pete teams, like everything that we start talking about. He's more of a to a guy. I think the Trevor Lawrence is the best quarterback in college football. I think he's going to be the most decorated college football player by the time  he goes to the draft a couple years from now and I think he's going to probably lead them to another National Championship in the next two seasons and he's gonna be as decorated as anyone probably since Tim Tebow that we've seen you have to take one of those for their NFL careers from what you've seen so far. Which one do we are you Takin Trevor Lawrence turtle or throats out. See I wish people is here because he just I don't see it. I don't see it as much Pete. Hope you download this and listen to this. I don't know how you can not see it. I'm  I like to but when I'm looking at guys within the lens of and maybe this is a personal preference thing to neck is I look at guys like Justin Herbert even about above to and that's just we're looking at you know arm strength. We're looking at me. He's we're looking at build. I mean we try not to because there's been people that have succeeded at the NFL level with a not typical build and people reference Drew Brees people reference Russell Wilson, but more times than not  it's people built like Trevor Lawrence that succeeded the NFL level and you see him. I mean God zip the ball. I mean, he's throwing he's throwing out patterns across the field on a I mean just zip it in there and to it listen to his made a lot of plays to a can keep plays alive. He's heavy. He's smart. He's tough and he's got he's got a ton of skills and he's going to be a first round pick, but if you're asked me one of those guys Trevor Lawrence with a bullet. Yeah could not agree with you more and I can just hope that our guy Pete Feud.  Is listening to this and shaking his head and disagreeing somewhere far away. Something is a look at nationally here obviously have to bring up the Big Ten that's against that's local for both of us. Are you buying the Michigan hype not until they actually do something right in the time that the time they had to do it was when they had was the peppers team two years ago. That was the squad. I had pushed all my chips in on I was like, these guys are make the playoffs. This is the team that's going to do it and  You know, I get another lost Ohio State. So that was by far the most talented team. They've had under Harbaugh and with all the losses on defense this year, you know, Devin Bush and when a bitch and I mean she Patterson didn't really show me much last year. I just I'm kind of like in a wait-and-see mode with them. I'm going to say they're not going to do it until they actually do it.  It's where I'm at with it too. It's just I get it. They don't lose games. They're not supposed to lose and there is something to be said for that but they don't beat tell me when Jim Harbaugh has beaten a team at Michigan that has anywhere near as much talent as him.  But it hasn't happened yet. Right and it's like I just look at it and it's okay like in that schedule is yeah, they get Notre Dame at home. You have to get Ohio State at home like that Wisconsin game. They have to travel to Wisconsin. That's not going to be easy to Penn State and Penn State's kind of in a rebuild type of season going on with them. But you get Penn State middle towards the end of the year like that's gonna be a different team than it is going to be like if you were playing them in September or late September and I think that's kind of where I just look at Michigan team and until they actually show me.  Can't get on board with it. So that mean you're full on board with Ohio State because I threw one by Pete who I think's going to win the Big Ten and it wasn't either one of those is it was at Ohio State for you, then that you're still fully on board with when in the Big Ten I was looking if I was looking to Futures that's for Michigan State. Thank you. Call me crazy. I didn't realize that with you. But yeah, that's what I told Pete the other day and he did you really like I had three heads. I told him like when you have the best defense in the conference, I don't think that's much of a question. Maybe the you can look at Ohio State and Sarah.  Has more on a felt Allen on their defense, there's not a better defensive unit the Michigan State in this conference and tell me which coach in the Big Ten you take over dantonio because I don't think there is one and now that's not to say that Ryan Day can't turn into that. He's not that yet. It's you mix the best events with the best coach in offense that I just don't think can be anywhere near as bad as it was last year not to say it's gonna be anything special and I'll take my ten to one twelve to one chances on that.  And if you remember before last season which Big Ten quarterback in a me a big-time quarterback that was getting more love than Brian low working.  There wasn't many know if any and then he was banged up all last year. He was not healthy at any point during last season if he's healthy now. That's a team that is it 17 starters back. Dantonio typically has that team play better than the talent on the field on the talent. They have at the beginning of year. They always they always outplay their talent and that's a team that I often look to from looking for a team to exceed expectations. You know, I look at Iowa is it seems like every other year I  Has a good year or they and then they go down and they go back up. They go down a little bit always kind of hovering around 9 and 3 6 & 6 but with Michigan State before that 2016 kind of blip on the radar last year. They definitely underplayed but aside from that they've constantly out performed and like you said, this is the best events of the Big Ten. Maybe not the most talented but the best unit in the Big Ten and then if it will work, he is healthy. They're going to be a problem and Michigan  They're not afraid to play Michigan at in Ann Arbor. No not whatsoever. And yes, they have to go to Ohio State. They have to go to Wisconsin and back-to-back weeks. I'm not saying they're going to run the table in the Big Ten, but I think they have plenty of talent. They have a pain in the ass defense that's going to I feel like knock one of these teams off if not two of them and all of a sudden they're right there in the running. And when this whole thing that's been made all offseason of owes us to your Ohio State finally Falls to Michigan. Can the Wolverine trying to get by like, Michigan.  Not being only the best team in their own damn State let alone their own division or conference. I I'm I don't I look at Michigan State and I just think that's the formula of if you want success that that is that's what you're looking at. And that's a team that I think is going to surprise and turn a lot of heads this season looking out a little bit more nationally Big 12 wise Oklahoma second year in a row. They replace the first overall pick in the Heisman Trophy winner. How much is the fall off there? Because it's not just that it's an offensive line that losses for starters as well. Is is this  Like finally the time that Lincoln Riley gets to deal with the the tough like overhaul of a roster college football wise or is it just a machine that just kind of reloads and keeps going for them. I think of that conference, especially when you eat do have if you have a talent at quarterback and that Lincoln Riley offense and as much as Jalen hurts, his Moline during his time in bama, he's a talent and he's a he's a highly rated recruit for a reason he put up numbers for a reason. I think he's going to be I think they're going to be just fine CD lamb are  I believe the biggest play maker in the country on the outside. They're stacked at tailback. They create Humphreys. Maybe the best player in the Big 12 in the office of why that's Center. They have replacing everybody else. But they've done that before they Oklahoma gets guys. That's that's not what they're framed offensively defensive never really their calling card. It's it hasn't been for quite some time since early in the Stoops era, but they're going to put up points and they're going to that's in Big 12 team that puts up.  Most points off as most successful and I don't see anyone that schedule right now outside of I mean people are blown up Texas. It almost seems like the Texas train came a year early and now everyone's jumping on board maybe a little too. Maybe they come back to earth a little bit this year. But TCU is the team I think in the Big 12, that is get maybe the give Oklahoma some problems. That's that's the squad I'm looking at right now that could give them as problems. Okay? Why do you like to see you so much because I have somebody else in the Big 12 that I am very high on. It's one of the it's the coaching Trust.  With Patterson and it's they often, you know, they've had at least 11 wins and three of The Last Five Years. They've got got 12 starters back which isn't isn't a lot but again, like I said see the lamb before Jalen reger's maybe 1A or 1B to him in this first Playmakers in the in the conference and they to experience its experienced team. They've been through the ringer Patterson is the best is my estimation the best overall coach of the Big 12 and a lot of times if there's  have some tea my sort of like sort of don't like I lean toward the coach and there's no one really on that TCU schedule that I could see them at. It's the at Oklahoma game, but again jumps out to me and I don't know. I don't know why this year when I look at the big 12, it's really just Oklahoman TCU then I'm looking at yeah, Texas is going to be good. But I do expect them to take a little bit of a step back from their from their last season who's the team that you're looking at like I like Iowa State are you? Okay, you mentioned the coaching part of it and I common trend for me to that's why I like Michigan State so much is largely because of their coach, but I look at Iowa State.  Ensure you have to replace in all World running back last year in Montgomery, but you had probably the best freshman quarterback. That wasn't named Trevor Lawrence in the entire country year experience with him. You get T. See you at home you get Texas at home for that team. I think that that's one that Matt Campbell right now. I expect him to be in the NFL before too long and a Lincoln Riley's name is always the one of the big 12 that gets thrown out there. Like I look at Campbell is a guy that's going to be coaching on an NFL sideline before long if he wants to if not that he's going to go to a  Big-time college program that's not names Iowa, and I kind of combined all of that a favorable schedule and I look at that as a team that's has a damn good shot to be played in the Big 12 championship and they've got the best front seven in the conference. They either offensive line and returns all five starters and they've got the best front seven of Defense the conference micros ji Quan Bailey two guys that are going to be are going to be hopefully more known this season is to go through I like Iowa State a lot too and I agree with you on Campbell. I mean that guy is guy's a star and he's going to be a bigger job sooner than later.  Yeah, that's the part that I see on it. And it's also they get Iowa at home. Not that that's important for for in terms of deciding the Big 12, but at least in terms of a rivalry game and a game that's going to give them a little bit more a little bit more exposure and they beat Iowa early on I just kind of going through conferences here Pac-12 when you look at that in terms of talent, they haven't won a big game out of conference. I don't know the last one probably last time Stanford beat Notre Dame. I can probably be the last one that's considered but about right. Yeah, I  It just seems like it's few and far between when they get a marquee win against an out of conference opponent organ takes on Auburn this year, otherwise chances. There aren't a whole heck of a lot of them. How does it work in stack up against Auburn and how does this conference kind of Stack up to the rest of the power fives? I mean for getting it seems like there's a lot of people on the end organs bandwagon this year two to one of the favored they return 17 starters. I mean Justin Herbert to the how you want to look at a when he would have been your first quarterback had he gone  Have you gone out this year? We have been your first quarterback had he gone out to the draft. Yes.  Yeah, and that's based on I mean it's a lot of its based on projection because those people point to his completion percentage and say he hasn't completed up passes for me is not as consistent as you like. I just think he has the look of a guy that and you dig in because you didn't do the extensive NFL work on emerged as extensive film work on them because he didn't come out but he's a guy that consistently made plays outside the pocket inside the pocket. He's playing in front of her and back of one of the best offensive lines in the country. So that gives him a little clearance but  Yeah, you made plays and he's he has the look of a quarterback and has the has the tools of the quarterback that I like. So yeah, he would have been the number one guy. I would have taken this year and that offensive line is something special maybe the best offensive line of the country. Troy die die on defense has a star at linebacker. They have Playmakers in this outside. Although I'm still waiting for jajuan Johnson the transfer for Penn State to make a play. It seems like people have been talking about him for three years and he still has done anything, so waiting on that to happen, but if there's  As a team and I don't know how you feel about this. But if there's a team that I'm looking to make a move in the Pac-12 this year, it's Utah if Tyler Huntley is healthy at quarterback. I think that's the team with the best best front for one of the best front force in the country. And the Jaylen Johnson who I expect to be a first round pick a corner next year. I think that's a team that is going to be in Kyle Whittingham again, we go back to the coaching thing. It's just a trusted coach a guy that gets the most out of his team's so I like Utah and that conference, but other than that, I mean,  Looking for USC to rebound Washington loses a jake Browning for the first time in six years. He's not gonna be starting quarterback for them. And then Stanford is always pretty good under Shaw Washington State loses Minshew a quarterback. So still a lot of unknowns. No one's really solidified themselves at the as the consistent presence and the presence in that conference. You know is Colorado one of these teams under Mel Tucker that's going to make a comeback, you know, her men were just still trying to do something Arizona State, but you toss the squad that I'm looking at this here to possibly do some that the Pac-12 so I don't know.  I don't know how you're looking at that but I that's that's the team that I would be picking if anyone is going to come out of there and go to the playoff. I'd be looking at Utah. Yeah, I think Utah is probably should I put this I don't want to say safe pick. I mean, you're the Utah Utes, but it's kind of playing in the south is favorable to them. Last year was such a weird year because quarterback gets hurt. They had who else got hurt for them last year. I'm trying to remember the running back no running back. Yeah. They both go down. You think their season is over and all of a sudden they take off and up winning the South their offense is depleted by  Have to get to the conference Championship in one of the uglier Conference Championship Games. You'll see they still went down to the wire against Washington in that game. Then they blew a big lead against Northwestern and their bowl game. So it's one of those of like, it feels like last year was a nice pleasant surprise for them, but I don't I don't think that they're I think Oregon or Washington one of those teams has the potential to be the best team. However, they also play with you know, beating up on each other in playing in the same division. And by the time you get to the  conference Championship Game those guys could simply be playing for a Rose Bowl berth when Utah would have a potential Spot Shot at the playoffs on the line that yeah, if there's a team that I think would be able to earn a spot to the playoffs based off of schedule based off of where they'd be sitting entering Conference Championship weekend. I think I agree with you on it being Utah. I don't know if I buy them being the best team. I think that there's more Talent than one Washington and organs rosters if that makes any sense a little bit. I agree with you there because a lot of my a lot of my picking, Utah.  Is the coach factor in the The Experience factor that because I think there's a lot built into that Utah program with crystal ball in his second year at Oregon. Just so kind of waiting to see what is he going to what is he going to solidify their for the Ducks? But if you're looking at the team with the best quarterback in the best Playmakers and the best offensive line now, I'm just kind of selling Oregon for a person that didn't even take or again. I'm going to sell the organ for you, but that'd be that's that's a good selection in Washington is going to be interesting because I think Chris Peterson has  I think he's he's kind of got that program where they could they'd is this the year. This will be the test here. Is this a program now where you know, they're going to be good here. And you're out they they have been consistently bringing in guys bring it in guys. And I think this is the first year without browning without Gaskin, you know, a lot of that a lot of the face of that Squad is gone now and I know they I know because of injury They're bringing back the bring back Tre Adams on the offensive line. Aaron for is a  Budweiser receiver and absolute stud a wide receiver, but you know there it's the test here for Washington. I really want to see what happens with them. I'm very curious because I was a huge fan of them. I know we pick them much higher than they were when they made the playoffs in 16. I know we pinpointed them as a team to watch that year. They played outstanding but I think what's interesting is again if we're talking about coaches with with him and Whittingham and who else dantonio me and there's a lot of guys that because of their body of work.  They have garnered more trust in people that watch college football as far as when you're making progress prognostications for the year. Yeah, and it's you always expect him to get better and improve his a year goes on and it's always feels like a safe bet it's absurd that like you look at Utah and now they're packed fall for a good decade or so now so it has a different feel but like this was little old Utah 15 years ago when Whittingham took over for for a guy named Urban Meyer like oh, yeah, the old talks about maybe you talking crash the BCS. Urban Meyer got him to do that. That's like  Well, maybe they win one of these games against the big time team and Whittingham got them to do that. They beat Alabama in the Sugar Bowl the one you're all of a sudden. It's like it's a Pac-12 team and you're realistically looking at it before the year of like wouldn't be that huge of an upset of the Utah Utes earn a college football playoff. It's just it's insane of like little Utah has come and turned into this and it's just it's kind of weird of looking back of what Utah Sports words. Like I was Michael doleac and Keith Van Horn steam that wasn't a football program whatsoever when I was growing up and now it's all of a  in this there. Anyway, Rick Majerus. Yeah, Rick Majerus in is sweater. And yeah, I still look at the Utah. I look at in the Council of previews. I turned like the Mountain West and so expect see Utah there same way. Like my dad always when I was growing up. He would always call Memphis. Memphis State is a huge college basketball fan and I all Memphis State this Memphis State. They're like that. It's Memphis. There's no State on the end. He wouldn't understand. I'm same way the Utah. I was like, oh, yeah, they are in the Pac-12. Oh, yeah Colorado's in  At 12:00 to 1:00 the hell did that happen? Oh my goodness. Look at that. It non-power five teams. Notre Dame is is a team that I think the program is in better shape than it was a season ago at least entering the year, but the way that the roster kind of breaks down the way they have holes. I don't have good things for the Fighting Irish happening in Georgia and kind of being out of the playoff by then outside of that any other of the non power five teams have a chance at knocking on the door making things interesting and  Causing chaos for the for the final four for the college football playoff to you me eyes. Always I think the first power 5 T my eyes always drift toward when they're looking at previews for the seasons Boise State and that just shows you kind of the name brand that they've solidified in college football more Notre Dame. I'm the same way as you and I know we can I'm an ordained fan. So I will say that I often look in that look at them more negatively than I probably should but I do  You see them taking a bit of a step back this year and part of that is part of that is reality and part of that is being an ordained fan. You're kind of waiting for the fall back the next year that we really haven't been able string many seasons together that are consistently really really good. I think last year in the year before was really the first time that's happened in quite some time. I do think Ian books going to be good still some question that all of the wine is good every year but Kareem and aqua or possibly the best defensive end combo the country but Cody and tranquil  and Julian love and those are in Jerry Tillery up the middle. I mean those are huge losses and there's a lot of guys behind them that haven't seen much action because those guys played so much football. So a lot of questions there, but if there's one team and I think outside the power 5 that I'm looking at the possibly make some noise and people will point to Central Florida. But again, let me go back to this area for Brandon wimbush is at quarterback. I've got some questions with you, but throwing this thing becomes an issue. Yeah Cincinnati's the  That I'm looking at this year and I know Luke fickle and he'll be in his third year this year adam-11 to last year be Virginia Tech in the bowl game returning 14 starters. Dozen Ritter a quarterback is going to be a star maybe, you know potentially in like Lamar Jackson conversation coming up as a dual threat Michael Warren at tailback. Their defense is lights out led by James Wiggins at safety. A lot of guys that we're going to that we're going to hear about in the future and they're still pretty young to I think a lot of these guys will go back far enough.  Other your next year, so the Cincinnati team is not just going to be a problem this year, but Luke Vehicles got them pretty well set up in that American Conference for the future. They go to Ohio we to but other than that, I don't see I don't see a game at Memphis. The last Friday of the season is one of those underrated games of the year. I think that if both those teams are as good as they're supposed to be and that this is supposed to be good again. I think that's going to be a fantastic game to finish off the conference season before heading to the conference Championship games, but I  Today's a team. I could easily see in a going undefeated in conference into that. Memphis game the last game of the year and maybe have a again the problem though is with your if you're a non power find School most likely need an undefeated record going to get head into the playoff or make some noise in the playoff. If you're playing at Ohio State that second week of the season. I don't know if that's gonna work out so well for you, but Cincinnati's a team. I not only like for this year, but think pickles got the talent that program on the up and up and I look for big things.  For them in the next couple of years how long you think fickle lesser absolutely depends on what jobs open up. I mean, it's been a launching pad of late Tennessee to really have from their Notre Dame jobs come from they're fickle. Obviously the Ohio State ties doesn't seem like that's going to be open anytime soon. But like that has just been a launching pad to Big Time like and it's interesting too because seeing is that fickle it had that time at Ohio State has to help. Both of fire was out. Does he kind of look at it differently? He's had the taste of the big time as he went on maybe.  Hopefully this little morning still young guy very young. That'll have you turned 40 yet. But I wonder if he's going to be more patient seeing as how it was at Ohio State and it wasn't very successful. I believe they went six and six years there, but it's going to be interesting because I hope and a lot of times I really hope some of these you see this in college basketball team college football some of these guys that see success in one or two years will jump at the first power 5 school first powerful.  Good job that comes up form and you can understand it in some respects because it's a huge payday and they may not you never know. What happens the next year. So you got to take what you have. There are other people that slow played a little bit more and then they fall into the the good job and there's a really really good job at top Tier 1 they build up more of a resume. So I don't know about fickle. It's just interesting because I think he's seen how it is on the other side and maybe we'll take a different approach to it Jeff. Appreciate you taking the time. It's been a lot of fun. You told us earlier. It's bamas.  The rest of everyone is just kind of living in at this football season. What's your biggest surprise? You see coming here in 2019. Oh baby, I would say horrible gets fired surprise anymore. I mean I think fired would be a surprise. I think underachieve. I I'm with you. I think that that is I think under achieve that team has it written all over themselves. There's all this talk of oh my God shape Patterson, it's office. If it's some butter. It's a third offense in three different his last three years playing football. Like that's an  Use for NFL quarterbacks, but now it's going to be the Difference Maker for him at the Collegiate level. I think this is a team. They lost a lot of NFL Talent by the way, like I think this has regression written all over it and big-time disappointment in Arbor. Well, how about, how about Iowa's Iowa Michigan State in the Big Ten championship game sign me up for that. What year was that? Like twenty thirteen or fourteen where it was just the pounded out like 14 to 10. Come back, Michigan State win, sign me up one of those. I know that the Big 12 thing.  Exciting and I know that the SEC is putting up points more and more. I'm good for one of those every once in a while. I think the West is going to be a lot better this year talent-wise. I mean, I think Minnesota is better. I think produce better. I think Nebraska is going to be better. I think that's and northwestern's got the best quarterback they've ever had at the school. So other than what's not talk about Illinois, but I think the rest of that I think the rest of that division is going to be so much better than we've seen and yeah they may they may knock each other around a bit, but the tent not just  The one the one loss record hasn't been the problem that division. It's been the talent. They haven't had teams that you look at and go that team can beat anybody. I think this year. You're going to see the Big Ten West you're going to see a bit of a Resurgence as far as the talent goes in that cop and that division. Yeah. I know that Illinois is seen as a step behind everyone else there. But how close is it 1 through 6 with the other six teams there for you E. I mean, this is like you could take a dart and just throw it in. Okay. Yep. Nebraska is going to finish first, Iowa second and Minnesota third and it wouldn't surprise anyone you.  Tell me the Big Ten West could have any I would be most the only one I really be truly out of out of my shoes surprise for would be Illinois. The rest of my could see potentially going to the conference Championship game. I mean, this is probably going to be maybe maybe the weakest Wisconsin team. We've seen in quite some time. I mean Jonathan Taylor obviously still there, but you're breaking new quarterback tongue gone on defense, but they're mean they're consistently good the rest of the team Brahms got Purdue going Flex.  Got Minnesota back. I think that it's always come back with a ton this year. Northwestern's all those is always going to Northwestern but Illinois this is supposed to be this year is supposed to be the year. They're competing for a bowl this coming season. So it's kind of put up or shut up time for them.  How are you with the state of the line, you're in Illini guy, like stated them state of the lobby thing state of the rebuild. If you want to call it that like, where are you at with you later? I mean fandom included and just casually just frustration just I think people need to look at it realistically and I think loves was a play. I think it would have been different if love you come to Illinois right after he he was at the Bears. I think that's a different draw. I don't know how many of these kids are really  drawn to Illinois. Bye Lovie Smith. I was a fan of the higher just because it was doing something different. It wasn't it wasn't a real. It wasn't one of the first time they were interesting ever and that's the team went to the Rose Bowl was look I know in my life, but they were never even interesting. It's like love you at least brought that but it quickly wore off. I was on campus the day that they hired him and for for a work thing and I had to go into a meeting for a couple hours. I came out and the campus was plastered and Lovie Smith stuff and I had never seen anything like that with them only, Illinois.  Ball probably since the sugar bowl or that starts at the Rose Bowl, but this is this is they've got enough Talent on that team but schematically and strategically during games throughout the last three years. They just have looked they've been lacking and with the number the number of players you've seen move out of the program MJ Rivers this year, Isaiah Williams is gonna be a freshman quarterback Reggie. Corbin is going to be a stud stud again at tailback.  Offensive lines pretty good, but a lot of these guys have been this will be the third year that they're seeing significant playing time for innate last season. It's not crazy to me to see a two-game Improvement. It's not the Ohio State's not on their schedule. So that's a plus they do play Michigan, Michigan State, Wisconsin and Iowa, but getting six wins anymore is not a huge fan and I would I wouldn't be surprised to get to six wins. I would be shocked if they won the division.  Awesome. Well appreciate you taking the time Jeff. This was a ton of fun. Like I said, we'll be checking in with you throughout football season and I don't be a stranger. All right, Nick. Thanks for having me man. Good talking and I can't wait for the season to start Jeff fair. You've read his work and Pro Football Weekly college football news using his byline inside the pylon big-time college football fan college football junkie has tons of information on damn near any team that I brought up there as you can tell so make sure to follow all of his work as well at the fair.  And it Twitter thank you again for listening to the College Football news podcast. As always. I'm here. I'm dick Schaap Kowski Pete few Tech regularly joins me as well. He's the one whose baby this whole thing. Is he sure if you haven't already check out some of the previous episodes. If you're getting ready for the college football season some back there that you'll probably enjoy probably learn things about some of the bigger programs, Alabama, Michigan, Ohio State Notre Dame and more plenty of conference breakdowns and extensive previews as well as very great guests.  Time guests from big-time networks joined us to get you ready for the 2019 season until next time. I'm dick Schaap Kowski. Have a great rest of your afternoon evening or night whatever it may be and keep it tuned right here for the college football news podcast coming at you all football season long.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx34IdjPvb_5",
        "colab_type": "text"
      },
      "source": [
        "#First N and t5 w/ coreference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PN-fUg4xBPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en')\n",
        "neuralcoref.add_to_pipe(nlp)\n",
        "\n",
        "def coref_resolution(transcript):\n",
        "  doc = nlp(transcript)\n",
        "  return doc._.coref_resolved"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-maND-uyl2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def firstn_t5_coref(transcript, limit, threshold):\n",
        "  sentences = first_n(transcript, limit)\n",
        "  resolved_sentences = coref_resolution(sentences)\n",
        "  summary = t5_inference(resolved_sentences, threshold)\n",
        "  return summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2p2Ap5gzT7d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "800cf34e-04df-4155-da6f-50669047bfef"
      },
      "source": [
        "ext_abs['firstn_coref_summary'] = ext_abs.apply(lambda row: firstn_t5_coref(row['transcript'], 15, 7000), axis = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (601 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjcZ0MSJ12EG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed18a69e-d0ef-450b-eac5-ecac9963c9dc"
      },
      "source": [
        "rouge = Rouge()\n",
        "rouge_scores = rouge.get_scores(ext_abs['firstn_coref_summary'], ext_abs['episode_description'], avg=True)\n",
        "rouge_scores['rouge-1']['f'], rouge_scores['rouge-2']['f'], rouge_scores['rouge-l']['f']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.19460879330911204, 0.0948191636225971, 0.2267655938612496)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzVTfi9dHdSx",
        "colab_type": "text"
      },
      "source": [
        "Pretty much the same as without coref, only marginally better. So this says that we definitely do not need most of the transcript, and coref is a nice to have"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlUcq9kyCe8I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "6c43a316-a752-4b5a-ac8f-d245dd45e828"
      },
      "source": [
        "ext_abs.iloc[10]['firstn_coref_summary']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This week, Nick and Jeff discuss college football news on the College Football News Podcast. --- This episode is sponsored by  Anchor: The easiest way to make a podcast. https://anchor.fm/app Support this podcast: https://anchor.fm/collegefootballnews/support this podcast: https://anchor.fm/collegefootballnews/support this podcast: https://anchor.fm/collegefootballnews/support this podcast: https://anchor.fm/collegefootballnews/support this podcast: https://anchor.fm/collegefootballnews/support this podcast: https://anchor.fm/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3raHIjEACiEt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "d3756b6f-6071-4f16-f44f-59b7e7bb0920"
      },
      "source": [
        "ext_abs.iloc[4]['firstn_coref_summary']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"This is the first episode of The Other School Podcast. I talk about what it means to be a teenager and why you should keep listening! Thank you so much for your patience, thank you so much for tuning in and thanks for listening! Today's session is going to be a bit different from the norm. We will go over what they don't teach you at school and how to learn these essential lessons. From teenage Finance earning money to personality Improvement. Please leave us a review on Instagram or Twitter @UncleSharif Sharma Facebook Group: https://www.instagram.com/uncleSharif Sharma YouTube Channel: https://www.youtube.com/uncleSharif Sharma\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nSjzqw2CnQe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "b33f4c66-6226-4e86-9840-e2e38e7bd275"
      },
      "source": [
        "ext_abs.iloc[4]['episode_description']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Introduction | Show Formalities | What to Expect This show is all about real learning.Teaching youn what they dont teach you at school. Everything you struggle with as a teenager.And the basics of what should be taught at school.for more information email @unschoolpodcast@gmail.com Host ; Ankush Sharma\\xa0 '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-z5xkvBGrRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}